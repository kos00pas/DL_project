{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "pip install PyMuPDF"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWikwlQvcCDP",
    "outputId": "6d81462a-923e-4921-8d80-ddcd09584237",
    "ExecuteTime": {
     "end_time": "2024-12-11T12:48:54.403432Z",
     "start_time": "2024-12-11T12:48:40.648669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (1.25.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "pip install evaluate"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQOrxem0cVK0",
    "outputId": "ed04d791-0fb3-4ec3-8dfb-d1fe2f875a87",
    "ExecuteTime": {
     "end_time": "2024-12-11T12:48:23.527779Z",
     "start_time": "2024-12-11T12:48:19.715855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from evaluate) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from evaluate) (0.25.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.16.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (18.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.15.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\kos00\\anaconda3\\envs\\doa_env\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "pip install transformers torch datasets"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upVYqvHHc3js",
    "outputId": "c7c4d1b3-0533-47b2-e886-60e367633edb"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KkIig8c1c_iF",
    "outputId": "e74f01cb-e9c6-4fb4-ace6-3a332e62ac59"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.6)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=8855f2430e8dec4bd5c60a46204e19671a0b0ef94cfdcd4079b56a1afd918642\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jLMdW9KPdOp_",
    "outputId": "3ef7d0d1-3b37-4dac-b3f8-9c4f3202bf9d"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Block 1: Summarization with Enhanced Section Extraction and Pre-processing\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF for PDF handling\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import spacy\n",
    "\n",
    "# NLTK and spaCy downloads\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# High-level keywords for important sentence extraction\n",
    "high_level_keywords = [\"objective\", \"goal\", \"approach\", \"propose\", \"demonstrate\", \"result\", \"outcome\",\n",
    "                       \"find\", \"discover\", \"introduce\", \"overview\", \"conclude\", \"describe\", \"present\", \"model\"]\n",
    "\n",
    "# Text cleaning and sentence filtering function\n",
    "def clean_text(text):\n",
    "    # Basic text cleaning\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\([A-Za-z,0-9\\s&;]*\\)', '', text)\n",
    "    text = re.sub(r'\\[[0-9,; ]+\\]', '', text)\n",
    "    text = re.sub(r'(Figure|Table|Equation|Eq|Fig|Exhibit)\\s*\\d+', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Segment into sentences and filter by length and keywords\n",
    "    doc = nlp(text)\n",
    "    sentences = [\n",
    "        sent.text.strip() for sent in doc.sents\n",
    "        if len(sent.text.split()) > 5 and any(keyword in sent.text.lower() for keyword in high_level_keywords)\n",
    "    ]\n",
    "\n",
    "    # Limit to a concise number of sentences\n",
    "    return ' '.join(sentences[:5])\n",
    "\n",
    "# Enhanced Section extraction with improved regex flexibility\n",
    "def extract_sections_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\".join([doc.load_page(i).get_text(\"text\") for i in range(doc.page_count)])\n",
    "        doc.close()\n",
    "\n",
    "        # Regex for Introduction and Conclusion, allowing variations\n",
    "        intro_pattern = r\"(?:^|\\n)\\s*(Introduction|Objective|Background)\\s*[:.\\-]?\\s*(.*?)(?=\\n\\s*(Conclusion|Summary|Results|References|Acknowledgments|Appendix|$))\"\n",
    "        concl_pattern = r\"(?:^|\\n)\\s*(Conclusion|Summary|Closing Remarks|DISCUSSION & CONCLUSIONS|Final Thoughts)\\s*[:.\\-]?\\s*(.*?)(?=\\n\\s*(References|Acknowledgments|Appendix|$))\"\n",
    "\n",
    "        # Get matches within relevant parts of text\n",
    "        intro_match = re.search(intro_pattern, text[:int(len(text) * 0.3)], re.S | re.I)\n",
    "        concl_match = re.search(concl_pattern, text[-int(len(text) * 0.3):], re.S | re.I)\n",
    "\n",
    "        # Clean and filter based on matches or fallback to a default\n",
    "        introduction_text = clean_text(intro_match.group(2)) if intro_match else clean_text(text[:int(len(text) * 0.3)])\n",
    "        conclusion_text = clean_text(concl_match.group(2)) if concl_match else clean_text(text[-int(len(text) * 0.3):])\n",
    "\n",
    "        # Print debug information for extracted sections\n",
    "        print(f\"\\n[DEBUG] Introduction for {os.path.basename(pdf_path)}:\\n{introduction_text}\\n{'-'*80}\")\n",
    "        print(f\"\\n[DEBUG] Conclusion for {os.path.basename(pdf_path)}:\\n{conclusion_text}\\n{'-'*80}\")\n",
    "\n",
    "        return introduction_text, conclusion_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Generate summary using BART model, aiming for specified word length\n",
    "def generate_summary(text, model, tokenizer, min_words=200, max_words=300):\n",
    "    # Set min and max length in tokens, estimating 1.3 tokens per word on average\n",
    "    min_length = int(min_words * 1.3)\n",
    "    max_length = int(max_words * 1.3)\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        num_beams=5,\n",
    "        length_penalty=1.5,  # Balanced to prevent overly terse or verbose summaries\n",
    "        no_repeat_ngram_size=3,  # Avoids repetitive phrases for readability\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Filter out repetitive or incomplete sentences in summary\n",
    "def filter_summary(summary_text):\n",
    "    sentences = summary_text.split('. ')\n",
    "    unique_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence not in unique_sentences:\n",
    "            unique_sentences.append(sentence)\n",
    "    return '. '.join(unique_sentences)\n",
    "\n",
    "# Abstract extraction function with regex pattern\n",
    "def extract_abstract_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\".join([doc.load_page(i).get_text(\"text\") for i in range(min(2, doc.page_count))])  # Use only the first 2 pages for Abstract\n",
    "        doc.close()\n",
    "\n",
    "        # Regex pattern for Abstract section, allowing some variation\n",
    "        abstract_pattern = r\"(?:^|\\n)\\s*(Abstract)\\s*[:.\\-]?\\s*(.*?)(?=\\n\\s*(Introduction|Background|Conclusion|References|$))\"\n",
    "\n",
    "        # Extract abstract based on pattern match\n",
    "        abstract_match = re.search(abstract_pattern, text, re.S | re.I)\n",
    "        abstract_text = clean_text(abstract_match.group(2)) if abstract_match else \"Abstract not found.\"\n",
    "\n",
    "        # Print debug information for extracted abstract\n",
    "        print(f\"\\n[DEBUG] Abstract for {os.path.basename(pdf_path)}:\\n{abstract_text}\\n{'-'*80}\")\n",
    "\n",
    "        return abstract_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "        return None\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GF7F_Y47l5ta",
    "outputId": "87ef6cfb-3ac9-45e4-8f49-f600dfa71cb2"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Summarize PDFs in a directory and display abstracts\n",
    "def extract_abstracts_from_pdfs(pdf_directory):\n",
    "    for filename in os.listdir(pdf_directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            abstract = extract_abstract_from_pdf(os.path.join(pdf_directory, filename))\n",
    "            if abstract:\n",
    "                print(f\"\\n[DEBUG] Abstract for {filename}:\\n{abstract}\\n\")\n",
    "\n",
    "# Relevance comparison between abstract and enhanced summary\n",
    "def compare_abstract_with_summary(abstract, summary):\n",
    "    if not abstract or abstract == \"Abstract not found.\":\n",
    "        return \"Abstract not available for comparison.\"\n",
    "\n",
    "    # Calculate basic similarity by overlapping words\n",
    "    abstract_words = set(abstract.lower().split())\n",
    "    summary_words = set(summary.lower().split())\n",
    "    common_words = abstract_words.intersection(summary_words)\n",
    "    relevance_score = len(common_words) / len(abstract_words) if abstract_words else 0\n",
    "    relevance_percentage = relevance_score * 100\n",
    "\n",
    "    return f\"Relevance Score: {relevance_percentage:.2f}% - Common words: {len(common_words)} / {len(abstract_words)}\"\n",
    "\n",
    "# Generate enhanced summary and compare with abstract\n",
    "def extract_sections_and_summarize(pdf_path, model, tokenizer):\n",
    "    intro, concl = extract_sections_from_pdf(pdf_path)\n",
    "    if intro and concl:\n",
    "        combined_text = f\"Introduction: {intro} Conclusion: {concl}\"\n",
    "        summary = generate_summary(combined_text, model, tokenizer)\n",
    "        return filter_summary(summary)\n",
    "    return None\n",
    "\n",
    "# Summarize and compare PDFs in a directory\n",
    "def summarize_and_compare_pdfs(pdf_directory):\n",
    "    model_name = \"facebook/bart-large-cnn\"\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    for filename in os.listdir(pdf_directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_directory, filename)\n",
    "\n",
    "            # Extract abstract\n",
    "            abstract = extract_abstract_from_pdf(pdf_path)\n",
    "            print(f\"\\n[DEBUG] Abstract for {filename}:\\n{abstract}\\n{'-'*80}\")\n",
    "\n",
    "            # Generate enhanced summary\n",
    "            enhanced_summary = extract_sections_and_summarize(pdf_path, model, tokenizer)\n",
    "            print(f\"\\n[DEBUG] Enhanced Summary for {filename}:\\n{enhanced_summary}\\n{'-'*80}\")\n",
    "\n",
    "            # Compare abstract and enhanced summary\n",
    "            if enhanced_summary:\n",
    "                comparison_result = compare_abstract_with_summary(abstract, enhanced_summary)\n",
    "                print(f\"[DEBUG] Comparison for {filename}:\\n{comparison_result}\\n{'-'*80}\")\n",
    "\n",
    "# Run the summarization and comparison block\n",
    "pdf_directory = \"/content/drive/My Drive/10\"\n",
    "summarize_and_compare_pdfs(pdf_directory)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dFQN4QUSYY3",
    "outputId": "b7f99445-b180-4080-866d-4cec6cea8a37"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[DEBUG] Abstract for Rock_mechanics_modeling_based_on_soft_granulation_theory.pdf:\n",
      "This paper describes application of information granulation theory on the design of rock engineering flowcharts Firstly an overall flowchart based on information granulation theory has been highlighted Information granulation theory in crisp nonfuzzy or fuzzy format can take into account engineering experiences especially in fuzzy shapeincomplete information or superfluous or engineering judgments in each step of designing procedure while the suitable instruments modeling are employed In this manner and to extension of soft modeling instruments using three combinations of Self Organizing Map  NeuroFuzzy Inference System  and Rough Set Theory  crisp and fuzzy granules from monitored data sets are obtained The main underlined core of our algorithms are balancing of crisprough or nonfuzzy granules and sub fuzzy granules within non fuzzy information  upon the openclose iterations Using different criteria on balancing best granules  are obtained Validations of our proposed methods on the data set of insitu permeability in rock masses in Shivashan dam Iran have been highlighted general network are affected from the several parameters concluded in granulation level factor In this paper we interest to tack in to account soft granulation in rock system Upon this by focusing in two categories C1 and C2 in figure1 we develop different soft granulation methods based on intelligent systems and approximate reasoning methods Added to this the bridging between hard and soft granulation is abstracted The most main distinguished facets of the soft granules are set theory interval analysis fuzzy set rough set Each of these theories considers part of uncertainty of information data words pictures Due to association of uncertainty and vagueness with the monitored data set particularly resulted from the insitu tests  accounting relevant approaches such probability Fuzzy Set Theory  and Rough Set Theory  to knowledge acquisition extraction of rules and prediction of unknown cases more than the past have been distinguished Zadeh has emphasized the role of FST in geosciences will be increased during future years The RST introduced by Pawlak has often proved to be an excellent mathematical tool for the analysis of a vague description of object   The adjective vague referring to the quality of information means inconsistency or ambiguity which follows from information granulation The rough set philosophy is based on the assumption that with every object of the universe is associated a certain amount of information expressed by means of some attributes used for object description The indiscernibility relation  which is a mathematical basis of the rough set theory induces a partition of the universe in to blocks of indiscernible objects called elementary sets which can be used to build knowledge about a real or abstract world Precise condition rules can be extracted from a discernibility matrix Application of RST in different fields of the applied sciences has been reported   but developing of such system  in rock engineering have not been outstanding relatively  shows a general procedure in which the IGT accompanies by a predefined project based rock engineering design After determination of constraints and the associated rock engineering considerations the initial granulation of information as well as numerical  or in linguistic formats is accomplished Improvement of modeling instruments based upon IGs whether in independent or affiliated shape with hard computing methods such fuzzy finite element fuzzy boundary element stochastic finite element are new challenges in the current discussion In this study under modeling instruments box we propose three algorithms namely successive elicitation of crisp nonfuzzy fuzzy and rough granulations Self Organizing NeuroFuzzy Inference System  in an abbreviated manner SONFISR SONFISAR and Self Organizing Rough Set Theory  Fig1 one of the last general flowcharts to rock engineering design In figure 3 we have concluded a summary of current overall granulation in a rock project that leads to the formation of fuzzy granules on the attributes  of joints  show how one usually employs granulation procedure to permeability analysis in a dam site instinctively The rest of paper has been organized as section 2 preliminaries on some soft granulation methods ie SOM NFIS and RST in next section we propose three main algorithms and part 4 covers a practical instance describes how the soft granules ensue a relatively complete analysis on the permeability of Shivashan dam site in Iran 2 PRELIMINARIES 21 Self Organizing feature Map  Kohonens SOM algorithm has been well renowned as an ideal candidate for classifying input data in an unsupervised learning way  Kohonen self organizing networks Kohonen feature maps or topologypreserving maps are competitionbased network paradigm for data clustering The learning procedure of Kohonen feature maps is similar to the\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for Rock_mechanics_modeling_based_on_soft_granulation_theory.pdf:\n",
      "This paper describes application of information granulation theory on the design of rock engineering flowcharts Firstly an overall flowchart based on information granulation theory has been highlighted Information granulation theory in crisp nonfuzzy or fuzzy format can take into account engineering experiences especially in fuzzy shapeincomplete information or superfluous or engineering judgments in each step of designing procedure while the suitable instruments modeling are employed In this manner and to extension of soft modeling instruments using three combinations of Self Organizing Map  NeuroFuzzy Inference System  and Rough Set Theory  crisp and fuzzy granules from monitored data sets are obtained The main underlined core of our algorithms are balancing of crisprough or nonfuzzy granules and sub fuzzy granules within non fuzzy information  upon the openclose iterations Using different criteria on balancing best granules  are obtained Validations of our proposed methods on the data set of insitu permeability in rock masses in Shivashan dam Iran have been highlighted general network are affected from the several parameters concluded in granulation level factor In this paper we interest to tack in to account soft granulation in rock system Upon this by focusing in two categories C1 and C2 in figure1 we develop different soft granulation methods based on intelligent systems and approximate reasoning methods Added to this the bridging between hard and soft granulation is abstracted The most main distinguished facets of the soft granules are set theory interval analysis fuzzy set rough set Each of these theories considers part of uncertainty of information data words pictures Due to association of uncertainty and vagueness with the monitored data set particularly resulted from the insitu tests  accounting relevant approaches such probability Fuzzy Set Theory  and Rough Set Theory  to knowledge acquisition extraction of rules and prediction of unknown cases more than the past have been distinguished Zadeh has emphasized the role of FST in geosciences will be increased during future years The RST introduced by Pawlak has often proved to be an excellent mathematical tool for the analysis of a vague description of object   The adjective vague referring to the quality of information means inconsistency or ambiguity which follows from information granulation The rough set philosophy is based on the assumption that with every object of the universe is associated a certain amount of information expressed by means of some attributes used for object description The indiscernibility relation  which is a mathematical basis of the rough set theory induces a partition of the universe in to blocks of indiscernible objects called elementary sets which can be used to build knowledge about a real or abstract world Precise condition rules can be extracted from a discernibility matrix Application of RST in different fields of the applied sciences has been reported   but developing of such system  in rock engineering have not been outstanding relatively  shows a general procedure in which the IGT accompanies by a predefined project based rock engineering design After determination of constraints and the associated rock engineering considerations the initial granulation of information as well as numerical  or in linguistic formats is accomplished Improvement of modeling instruments based upon IGs whether in independent or affiliated shape with hard computing methods such fuzzy finite element fuzzy boundary element stochastic finite element are new challenges in the current discussion In this study under modeling instruments box we propose three algorithms namely successive elicitation of crisp nonfuzzy fuzzy and rough granulations Self Organizing NeuroFuzzy Inference System  in an abbreviated manner SONFISR SONFISAR and Self Organizing Rough Set Theory  Fig1 one of the last general flowcharts to rock engineering design In figure 3 we have concluded a summary of current overall granulation in a rock project that leads to the formation of fuzzy granules on the attributes  of joints  show how one usually employs granulation procedure to permeability analysis in a dam site instinctively The rest of paper has been organized as section 2 preliminaries on some soft granulation methods ie SOM NFIS and RST in next section we propose three main algorithms and part 4 covers a practical instance describes how the soft granules ensue a relatively complete analysis on the permeability of Shivashan dam site in Iran 2 PRELIMINARIES 21 Self Organizing feature Map  Kohonens SOM algorithm has been well renowned as an ideal candidate for classifying input data in an unsupervised learning way  Kohonen self organizing networks Kohonen feature maps or topologypreserving maps are competitionbased network paradigm for data clustering The learning procedure of Kohonen feature maps is similar to the\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Introduction for Rock_mechanics_modeling_based_on_soft_granulation_theory.pdf:\n",
      "1 INTROUDCTION In the recent years developing new indirect analysis methods has opened new horizons in rock engineering solutions Tendency to the microview of the natural events in the rock based systems upon the high speed PC technology behind large scale investigations has allocated new challenges in this track Transition from a general  view in to the detailed descriptions can be interpreted as relations interextra relations of the commuted information packages got from accumulations of data experience novelty and other effective agents Such construction of the whole from part  is the current behavior of human cognition Among the basic concepts which underlie human cognition there are three remarkable sides which are granulation organization and causation Granulation involves decomposition of whole into parts organization involves integration of parts in to whole and causation relates to association of causes with effects  Under this view from the discritization meshing blocking latticing of the interior or boundary of a field to the solving steps  of the problem are the perspectives of granulation We called first level of granulation as hard granulation and second level as soft granulation To better understand of the meaning of hard and soft granulation we reproduce the general rock engineering design flowchart in figure1  Level1 can be supposed as a hard granulation where level2 is related with soft granulation Clearly in soft granulation we are approaching to the real human cognition whereas in hard packing the machine computations are distinguished Let us consider the last class in level2  internet based system Interestingly this category shows how the discriminated projects under the virtual world employ the distributed information granules Plainly the contributions of any projects and the subsets of granules in construction of this Rock mechanics modeling based on soft granulation theory This paper describes application of information granulation theory on the design of rock engineering flowcharts Firstly an overall flowchart based on information granulation theory has been highlighted Information granulation theory in crisp nonfuzzy or fuzzy format can take into account engineering experiences especially in fuzzy shapeincomplete information or superfluous or engineering judgments in each step of designing procedure while the suitable instruments modeling are employed In this manner and to extension of soft modeling instruments using three combinations of Self Organizing Map  NeuroFuzzy Inference System  and Rough Set Theory  crisp and fuzzy granules from monitored data sets are obtained The main underlined core of our algorithms are balancing of crisprough or nonfuzzy granules and sub fuzzy granules within non fuzzy information  upon the openclose iterations Using different criteria on balancing best granules  are obtained Validations of our proposed methods on the data set of insitu permeability in rock masses in Shivashan dam Iran have been highlighted general network are affected from the several parameters concluded in granulation level factor In this paper we interest to tack in to account soft granulation in rock system Upon this by focusing in two categories C1 and C2 in figure1 we develop different soft granulation methods based on intelligent systems and approximate reasoning methods Added to this the bridging between hard and soft granulation is abstracted The most main distinguished facets of the soft granules are set theory interval analysis fuzzy set rough set Each of these theories considers part of uncertainty of information data words pictures Due to association of uncertainty and vagueness with the monitored data set particularly resulted from the insitu tests  accounting relevant approaches such probability Fuzzy Set Theory  and Rough Set Theory  to knowledge acquisition extraction of rules and prediction of unknown cases more than the past have been distinguished Zadeh has emphasized the role of FST in geosciences will be increased during future years The RST introduced by Pawlak has often proved to be an excellent mathematical tool for the analysis of a vague description of object   The adjective vague referring to the quality of information means inconsistency or ambiguity which follows from information granulation The rough set philosophy is based on the assumption that with every object of the universe is associated a certain amount of information expressed by means of some attributes used for object description The indiscernibility relation  which is a mathematical basis of the rough set theory induces a partition of the universe in to blocks of indiscernible objects called elementary sets which can be used to build knowledge about a real or abstract world Precise condition rules can be extracted from a discernibility matrix Application of RST in different fields of the applied sciences has been reported   but developing of such system  in rock engineering have not been outstanding relatively  shows a general procedure in which the IGT accompanies by a predefined project based rock engineering design After determination of constraints and the associated rock engineering considerations the initial granulation of information as well as numerical  or in linguistic formats is accomplished Improvement of modeling instruments based upon IGs whether in independent or affiliated shape with hard computing methods such fuzzy finite element fuzzy boundary element stochastic finite element are new challenges in the current discussion In this study under modeling instruments box we propose three algorithms namely successive elicitation of crisp nonfuzzy fuzzy and rough granulations Self Organizing NeuroFuzzy Inference System  in an abbreviated manner SONFISR SONFISAR and Self Organizing Rough Set Theory  Fig1 one of the last general flowcharts to rock engineering design In figure 3 we have concluded a summary of current overall granulation in a rock project that leads to the formation of fuzzy granules on the attributes  of joints  show how one usually employs granulation procedure to permeability analysis in a dam site instinctively The rest of paper has been organized as section 2 preliminaries on some soft granulation methods ie SOM NFIS and RST in next section we propose three main algorithms and part 4 covers a practical instance describes how the soft granules ensue a relatively complete analysis on the permeability of Shivashan dam site in Iran 2 PRELIMINARIES 21 Self Organizing feature Map  Kohonens SOM algorithm has been well renowned as an ideal candidate for classifying input data in an unsupervised learning way  Kohonen self organizing networks Kohonen feature maps or topologypreserving maps are competitionbased network paradigm for data clustering The learning procedure of Kohonen feature maps is similar to the competitive learning networks The main idea behind competitive learning is simple the winner takes all The competitive transfer function returns neural outputs of 0 for all neurons except for the winner which receives the highest net input with output 1 SOM changes all weight vectors of neurons in the near vicinity of the winner neuron towards the input vector Due to this property SOM are used to reduce the dimensionality of complex data  Competitive layers will automatically learn to classify input vectors the classes that the competitive layer finds are depend only on the distances between input vectors  Fig2 A general methodology for back analysis based on IGT 22 Neurofuzzy inference system  There are different solutions of fuzzy inference systems Two wellknown fuzzy modeling methods are the Tsukamoto fuzzy model and Takagi SugenoKang  model In the present work only the TSK model has been considered The TSK fuzzy inference systems can be easily implanted in the form of a so called Neurofuzzy network structure in this study we have employed an adaptive neurofuzzy inference system  Fig3 Current overall granulation in a rock project Fig4 Granulation procedure to permeability analysis in a dam site One of the most important stages of the Neuro fuzzy TSK network generation is the establishment of the inference rules Often used is the socalled grid method in which the rules are defined as the combinations of the membership functions for each input variable If we split the input variable range into a limited number say ni for i1 2 n of membership functions the combinations of them lead to many different inference rules The problem is that these combinations correspond in many cases to the regions of no data and hence a lot of them may be deleted This problem can be solved by using the fuzzy selforganization algorithm\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Conclusion for Rock_mechanics_modeling_based_on_soft_granulation_theory.pdf:\n",
      "We have explained application of SORST in back analysis in other study  Fig9  SONFISR results with maximum number of rules is 4 and closeopen iterations is 10 c Answer of selected SONFISR on the test data Now we investigate direct application of RST and NFIS on the local coordinates of dam site  and lugeon values  to depict 3D Isosurfaces of lugeon variations diagrams Fig10  SONFISR results with maximum number of rules 4 and closeopen iterations 20c d SONFISR with 5 to 8 rules number variation and 10 closeopen iterations e Answer of selected SONFISR based on nr5 on the test data  shows the variation of the lugeon data in Z 1 to 5 which has been acquired by serving five condition attributes in RST figure 16 the symbolic values by 1D SOM 5 neurons The categories 1 to 5 state very low low medium high and very high respectively Number 6  characterizes ambiguity and unknown cases Fig11SONFISAR neuron growth  error fluctuations vs iteration 101 α  a number of rules nr 2 b nr3and c nr4 Fig12 SONFISAR neuron growth  error fluctuations vs iteration 8 α   number of rules 2a RMSEiteration b neuron growthiteration c RMSE neuron fluctuation congestion of points can be used as a balance hole  Matrix plot of crisp granules by 79 grid topology SOM after 500 epochs on the training data set Fig 14 Results of transferring attribute Z L RQD TWR and lugeon in three categories  by 1D SOM Fig15 SORSTR results on the lugeon data set a strength factor b error measure variations along strength factor updating and c 3D column perspective of error measure neuron changes  Rules on N26 selected among 696 objects by SORSTR To clarify of permeability changes in consequent part of rules the lower value on the symbolic lugeon values which have relatively similar category for example 123 or 23 or 345 have been considered With serving NFIS on such attributesX Y Z lugeon without scaling permeability variations in figures 18 has been portrayed In this step three MFs  for input parameters have been utilized In Consequent of comparison between the results of RST and NFIS one may interprets the variations in z 2 is the superposition of sub levels involved z1160 to 1200 by NFIS approximately So the compatibility of the results derived from RST and NFIS can be probed by comparison of figurs1718 The forecasted domainsdark colors in figure17 by RST have been coincided by same regions in figure 18 closely It must be noticed that the RST model hasnt covered the high permeability zones because of employing conservative way in estimation of decision part whereas the NFIS has exposed such possible territories The rate of lugeon variations or density of permeable parts distinguishes the zones with capability of possible spring or hole Such cavities in the dam structures discussed as karsts which are the main characteristics of the limestone deposits  To find out the correlation between effective parameters and procuring of valid patterns of the rock mass in the dam site one may employ the similar process of NFIS or RST to estimate alterations of RQD and TWR  The contrary outputs in some zones with general contextual associated rules about RQD and lugeon implicate to the relatively complex structures aboard the rock mass Apart from a few details comparison of results indicates three overall zones in the rock mass in first zone the theoretic rules such reverse relate 1 z  2  Dec  1 2 l in 2 3  rqd  2  Dec  1 3 z  3  l  2  rqd  1  Dec  3 4 l  2  twr  3  Dec  3 5 z  3  l  1  Dec  1 OR Dec  3 6 l in 1 2  twr  2  Dec  2 7 rqd  2  twr  3  Dec  2 OR Dec  3 8 z  1  rqd  1  Dec  2 c between RQD lugeon are satisfied but in other zones the said rule is disregarded  Results of transferring attributes in five categories by 1D SOM To finding out of the background on these major zones we refer to the clustered data set by 2D SOM with 79 weights in competitive layer figure 10c on the first set of the attributes The clustered and graphical estimation disclose suitable coordination relatively For example in figure 13b we have highlighted three distinctive patterns among lugeon and Z RQD TWR One of the main reasons of being such patterns in the investigated rock mass is in the definition of RQD Indeed with developing of new approaches in information theory and computational intelligence as well as soft computing approaches it is necessary to consider these approaches to better understand of natural events in rock mass Under this view and granulation theory we proposed two main algorithms to complete soft granules construction in not 11 mapping level Self Organizing NeuroFuzzy Inference System Random and Regular neuron growthSONFISR SONFISAR and Self Organizing Rough Set Theory The authors would like to appreciate from Prof Witold Pedrycz Department of Electrical and Computer Engineering University of Alberta Canada for his encouragements and assurances along writing this paper REFERENCES  Zadeh L A 1997 Toward a theory of fuzzy information granulation and its centrality in human reasoning and fuzzy logic Fuzzy sets and systems 90 111127  Jing L A2003 Review of techniques advances and outstanding issues in numerical modeling for rock mechanics and rock engineering Int J Rock Mech Min Sci 40283354  Zadeh L A 2005  From Search Engines to Question Answering SystemsThe Problems of World Knowledge Relevance and Deduction in WSEAS Fuzzy Systems Lisbon Portugal 111127\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Enhanced Summary for Rock_mechanics_modeling_based_on_soft_granulation_theory.pdf:\n",
      "In the recent years developing new indirect analysis methods has opened new horizons in rock engineering solutions. In soft granulation we are approaching to the real human cognition whereas in hard packing the machine computations are distinguished. The main underlined core of our algorithms are balancing of crisprough or nonfuzzy granules and sub fuzzy granules within non fuzzy information. Using different criteria on balancing best granules  are obtained. Validations of our proposed methods on the data set of insitu permeability in rock masses in Shivashan dam Iran have been highlighted. The most main distinguished facets of the soft granules are set theory interval analysis fuzzy set rough set. The rough set philosophy is based on the assumption that with every object of the universe is associated a certain amount of information expressed by means of some attributes used for object description. Due to association of uncertainty and vagueness with the monitored data set particularly resulted from the insitu tests  accounting relevant approaches such probability Fuzzy Set Theory and Rough Set Theory  to knowledge acquisition extraction of rules and prediction of unknown cases more than the past have been distinguished. In this paper we interest to tack in to accountsoft granulation in rock system. We develop different soft granulated methods based on intelligent systems and approximate reasoning methods. Added to this the bridging between hard and soft granulations is abstracted.\n",
      "--------------------------------------------------------------------------------\n",
      "[DEBUG] Comparison for Rock_mechanics_modeling_based_on_soft_granulation_theory.pdf:\n",
      "Relevance Score: 35.29% - Common words: 114 / 323\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for Incremental_Pruning__A_Simple__Fast__Exact_Method_for_Partially___Observable_Markov_Decision_Processes.pdf:\n",
      "Most exact algorithms for general par tially observable Markov decision processes  use a form of dynamic program ming in which a piecewiselinear and con vex representation of one value function is transformed into another We examine vari ations of the incremental pruning method for solving this problem and compare them to earlier algorithms from theoretical and em pirical perspectives We find that incremen tal pruning is presently the most efficient ex act method for solving POMDPs 1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for Incremental_Pruning__A_Simple__Fast__Exact_Method_for_Partially___Observable_Markov_Decision_Processes.pdf:\n",
      "Most exact algorithms for general par tially observable Markov decision processes  use a form of dynamic program ming in which a piecewiselinear and con vex representation of one value function is transformed into another We examine vari ations of the incremental pruning method for solving this problem and compare them to earlier algorithms from theoretical and em pirical perspectives We find that incremen tal pruning is presently the most efficient ex act method for solving POMDPs 1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Introduction for Incremental_Pruning__A_Simple__Fast__Exact_Method_for_Partially___Observable_Markov_Decision_Processes.pdf:\n",
      "Abstract Most exact algorithms for general par tially observable Markov decision processes  use a form of dynamic program ming in which a piecewiselinear and con vex representation of one value function is transformed into another We examine vari ations of the incremental pruning method for solving this problem and compare them to earlier algorithms from theoretical and em pirical perspectives We find that incremen tal pruning is presently the most efficient ex act method for solving POMDPs 1 INTRODUCTION Partially observable Markov decision processes  model decision theoretic planning problems in which an agent must make a sequence of decisions to maximize its utility given uncertainty in the effects of its actions and its current state Cassandra Kael bling  Littman 1994 White 1991 At any moment in time the agent is in one of a finite set of possible states S and must choose one of a finite set of possible actions This results in a new information state x7 defined by where xas  Przjs a EsES Prsls ax z Pr  Pr  L Przls a L PrsJs ax sES sES  Solving a POMDP means finding a policy 1r that maps each information state into an action so that the expected sum of discounted rewards is maximized 0 D 1 D 1 is the discount rate which controls how much future rewards count compared to nearterm re wards There are many ways to approach this prob lem based on checking which information states can be reached  search ing for good controllers  and using dynamic programming  Most exact algorithms for general POMDPs use a form of dynamic programming in which a piecewise linear and convex representation of one value func tion is transformed into another This includes algo rithms that solve POMDPs via value iteration  policy iteration  accelerated value iteration  structured representations  and ap proximation  Because dynamic programming updates are critical to such a wide ar ray of POMDP algorithms identifying fast algorithms is crucial Several algorithms for dynamicprogramming updates have been proposed such as one pass  exhaustive  linear support  and witness  Cheng  gave experimental evidence that the linear support algorithm is more efficient than the onepass algorithm Littman Cassandra and Kael bling  compared the exhaustive algorithm the linear support algorithm and the witness algorithm and found that except for tiny problems with approx imately 2 observations or 2 states which all three al gorithms could solve quickly witness was the fastest and had a number of superior theoretical properties Recently Zhang and Liu  proposed a new method for dynamicprogramming updates in POMDPS called incremental pruning In this paper we analyze the basic algorithm and a novel variation and com pare them to the witness algorithm We find that the incrementalpruningbased algorithms allow us to solve problems that could not be solved within reason able time limits using the witness algorithm 2 DP UPDATES The fundamental idea of the dynamicprogramming  update is to define a new value function V in terms of a given value function V Value functions are mappings from information states to expected dis counted total reward In valueiteration algorithms V incorporates one additional step of reward compared to V and in infinitehorizon algorithms V represents an improved approximation that is closer to the opti mal value function The function V maps information states to values and is defined by V  E z rax  Y L PrVxƇ  sES zEZ\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Conclusion for Incremental_Pruning__A_Simple__Fast__Exact_Method_for_Partially___Observable_Markov_Decision_Processes.pdf:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Enhanced Summary for Incremental_Pruning__A_Simple__Fast__Exact_Method_for_Partially___Observable_Markov_Decision_Processes.pdf:\n",
      "None\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for Relational_Dynamic_Bayesian_Networks.pdf:\n",
      "Stochastic processes that involve the creation of objects and relations over time are widespread but relatively poorly studied For example accurate fault diagnosis in factory assembly processes requires inferring the probabilities of erroneous assembly operations but doing this efﬁciently and accurately is difﬁcult Modeled as dynamic Bayesian networks these processes have discrete vari ables with very large domains and extremely high dimensionality In this paper we introduce relational dynamic Bayesian networks  which are an extension of dynamic Bayesian net works  to ﬁrstorder logic RDBNs are a generalization of dynamic probabilistic relational models  which we had proposed in our previous work to model dynamic uncertain do mains We ﬁrst extend the RaoBlackwellised particle ﬁltering described in our earlier work to RDBNs Next we lift the assumptions associated with RaoBlackwellization in RDBNs and pro pose two new forms of particle ﬁltering The ﬁrst one uses abstraction hierarchies over the predi cates to smooth the particle ﬁlters estimates The second employs kernel density estimation with a kernel function speciﬁcally designed for relational domains Experiments show these two methods greatly outperform standard particle ﬁltering on the task of assembly plan execution monitoring 1 Introduction Sequential phenomena abound in the world and uncertainty is a common feature of them Dynamic Bayesian networks  one of the most powerful representations available for such phenomena represent the state of the world as a set of variables and model the probabilistic dependencies of the variables within and between time steps  While a major advance over previous approaches DBNs are essentially propositional with no notion of objects or relations hence DBNs are unable to compactly represent many realworld domains For example manu facturing plants assemble complex artifacts eg cars computers aircraft from large numbers of component parts using multiple kinds of machines and operations Capturing such a domain in a DBN would require exhaustively representing all possible objects and relations among them which is impractical Formalisms that can represent objects and relations as opposed to just variables have a long history in AI Recently signiﬁcant progress has been made in combining them with a principled treatment of uncertainty In particular probabilistic relational models or PRMs  are an extension of Bayesian networks that allows reasoning with classes objects and relations Recently we proposed dynamic probabilistic relational models   which combine PRMs and DBNs to allow reasoning with classes objects and relations in a dynamic environment We also developed a relational Rao Blackwellized particle ﬁltering mechanism for state monitoring in DPRMs c2005 In this paper we introduce relational dynamic Bayesian networks  which extend DBNs to ﬁrstorder  domains RDBNs subsume DPRMs and have several advantages over them including greater simplicity and expressivity Furthermore they may be more easily learned using ILP techniques We develop a series of efﬁcient inference procedures for RDBNs  The RaoBlackwellised particle ﬁltering described in our previous paper requires two strong assumptions which restrict its applicability We lift these assumptions developing two new forms of particle ﬁltering In the ﬁrst approach we build an abstraction hierarchy over the ﬁrstorder predicates and use it to smooth the particle ﬁlter estimates In the second approach we introduce a variant of kernel density estimation with a kernel function speciﬁcally designed for relational domains Early fault detection can greatly reduce the cost of manufacturing processes In this paper we apply our inference algorithms to executionmonitoring of assembly plans showing that our methods scale to problems with over a thousand objects and thousands of steps Other domains where our techniques may be helpful include robot control vision in motion language processing computational modeling of markets battleﬁeld management cell biology ecosystem modeling and analysis of Web information The following are the signiﬁcant contributions of this paper  We deﬁne relational dynamic Bayesian networks  which allow modeling uncer tainty in dynamic relational domains  We present several novel methods for inferencing in RDBNs which use RaoBlackwellization particle ﬁltering smoothing on relational abstraction hierarchies and relational kernel density estimation  We apply RDBNs to fault diagnosis in factory assembly processes showing that the inference algorithms we propose outperform traditional particle ﬁltering The rest of the paper is structured as follows In Section 2 we review DBNs and brieﬂy discuss the different ﬁltering algorithms applicable to them We introduce RDBNs in Section 3 and in Sections 4 5 and 6 we describe our inference methods In Section 7 we report our experimental results In Section 8 we discuss the related work and we show that RDBNs subsume DPRMs We conclude with a discussion of future work 2 Background A Bayesian network encodes the joint probability distribution of a set of variables Z1     Zd as a directed acyclic graph and a set of conditional probability models Each node corresponds to a variable and the model associated with it allows us to compute the probability of a state of the variable given the state of its parents The set of parents of Zi denoted Pa is the set of nodes with an arc to Zi in the graph 21 Dynamic Bayesian Networks Dynamic Bayesian Networks   are an extension of Bayesian net works for modeling dynamic systems In a DBN the state at time t is represented by a set of random 760\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for Relational_Dynamic_Bayesian_Networks.pdf:\n",
      "Stochastic processes that involve the creation of objects and relations over time are widespread but relatively poorly studied For example accurate fault diagnosis in factory assembly processes requires inferring the probabilities of erroneous assembly operations but doing this efﬁciently and accurately is difﬁcult Modeled as dynamic Bayesian networks these processes have discrete vari ables with very large domains and extremely high dimensionality In this paper we introduce relational dynamic Bayesian networks  which are an extension of dynamic Bayesian net works  to ﬁrstorder logic RDBNs are a generalization of dynamic probabilistic relational models  which we had proposed in our previous work to model dynamic uncertain do mains We ﬁrst extend the RaoBlackwellised particle ﬁltering described in our earlier work to RDBNs Next we lift the assumptions associated with RaoBlackwellization in RDBNs and pro pose two new forms of particle ﬁltering The ﬁrst one uses abstraction hierarchies over the predi cates to smooth the particle ﬁlters estimates The second employs kernel density estimation with a kernel function speciﬁcally designed for relational domains Experiments show these two methods greatly outperform standard particle ﬁltering on the task of assembly plan execution monitoring 1 Introduction Sequential phenomena abound in the world and uncertainty is a common feature of them Dynamic Bayesian networks  one of the most powerful representations available for such phenomena represent the state of the world as a set of variables and model the probabilistic dependencies of the variables within and between time steps  While a major advance over previous approaches DBNs are essentially propositional with no notion of objects or relations hence DBNs are unable to compactly represent many realworld domains For example manu facturing plants assemble complex artifacts eg cars computers aircraft from large numbers of component parts using multiple kinds of machines and operations Capturing such a domain in a DBN would require exhaustively representing all possible objects and relations among them which is impractical Formalisms that can represent objects and relations as opposed to just variables have a long history in AI Recently signiﬁcant progress has been made in combining them with a principled treatment of uncertainty In particular probabilistic relational models or PRMs  are an extension of Bayesian networks that allows reasoning with classes objects and relations Recently we proposed dynamic probabilistic relational models   which combine PRMs and DBNs to allow reasoning with classes objects and relations in a dynamic environment We also developed a relational Rao Blackwellized particle ﬁltering mechanism for state monitoring in DPRMs c2005 In this paper we introduce relational dynamic Bayesian networks  which extend DBNs to ﬁrstorder  domains RDBNs subsume DPRMs and have several advantages over them including greater simplicity and expressivity Furthermore they may be more easily learned using ILP techniques We develop a series of efﬁcient inference procedures for RDBNs  The RaoBlackwellised particle ﬁltering described in our previous paper requires two strong assumptions which restrict its applicability We lift these assumptions developing two new forms of particle ﬁltering In the ﬁrst approach we build an abstraction hierarchy over the ﬁrstorder predicates and use it to smooth the particle ﬁlter estimates In the second approach we introduce a variant of kernel density estimation with a kernel function speciﬁcally designed for relational domains Early fault detection can greatly reduce the cost of manufacturing processes In this paper we apply our inference algorithms to executionmonitoring of assembly plans showing that our methods scale to problems with over a thousand objects and thousands of steps Other domains where our techniques may be helpful include robot control vision in motion language processing computational modeling of markets battleﬁeld management cell biology ecosystem modeling and analysis of Web information The following are the signiﬁcant contributions of this paper  We deﬁne relational dynamic Bayesian networks  which allow modeling uncer tainty in dynamic relational domains  We present several novel methods for inferencing in RDBNs which use RaoBlackwellization particle ﬁltering smoothing on relational abstraction hierarchies and relational kernel density estimation  We apply RDBNs to fault diagnosis in factory assembly processes showing that the inference algorithms we propose outperform traditional particle ﬁltering The rest of the paper is structured as follows In Section 2 we review DBNs and brieﬂy discuss the different ﬁltering algorithms applicable to them We introduce RDBNs in Section 3 and in Sections 4 5 and 6 we describe our inference methods In Section 7 we report our experimental results In Section 8 we discuss the related work and we show that RDBNs subsume DPRMs We conclude with a discussion of future work 2 Background A Bayesian network encodes the joint probability distribution of a set of variables Z1     Zd as a directed acyclic graph and a set of conditional probability models Each node corresponds to a variable and the model associated with it allows us to compute the probability of a state of the variable given the state of its parents The set of parents of Zi denoted Pa is the set of nodes with an arc to Zi in the graph 21 Dynamic Bayesian Networks Dynamic Bayesian Networks   are an extension of Bayesian net works for modeling dynamic systems In a DBN the state at time t is represented by a set of random 760\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Introduction for Relational_Dynamic_Bayesian_Networks.pdf:\n",
      "Submitted 1004 published 1205 Relational Dynamic Bayesian Networks Sumit Sanghai SANGHAICSWASHINGTONEDU Pedro Domingos PEDRODCSWASHINGTONEDU Daniel Weld WELDCSWASHINGTONEDU Department of Computer Science and Engineering University of Washington Abstract Stochastic processes that involve the creation of objects and relations over time are widespread but relatively poorly studied For example accurate fault diagnosis in factory assembly processes requires inferring the probabilities of erroneous assembly operations but doing this efﬁciently and accurately is difﬁcult Modeled as dynamic Bayesian networks these processes have discrete vari ables with very large domains and extremely high dimensionality In this paper we introduce relational dynamic Bayesian networks  which are an extension of dynamic Bayesian net works  to ﬁrstorder logic RDBNs are a generalization of dynamic probabilistic relational models  which we had proposed in our previous work to model dynamic uncertain do mains We ﬁrst extend the RaoBlackwellised particle ﬁltering described in our earlier work to RDBNs Next we lift the assumptions associated with RaoBlackwellization in RDBNs and pro pose two new forms of particle ﬁltering The ﬁrst one uses abstraction hierarchies over the predi cates to smooth the particle ﬁlters estimates The second employs kernel density estimation with a kernel function speciﬁcally designed for relational domains Experiments show these two methods greatly outperform standard particle ﬁltering on the task of assembly plan execution monitoring 1 Introduction Sequential phenomena abound in the world and uncertainty is a common feature of them Dynamic Bayesian networks  one of the most powerful representations available for such phenomena represent the state of the world as a set of variables and model the probabilistic dependencies of the variables within and between time steps  While a major advance over previous approaches DBNs are essentially propositional with no notion of objects or relations hence DBNs are unable to compactly represent many realworld domains For example manu facturing plants assemble complex artifacts eg cars computers aircraft from large numbers of component parts using multiple kinds of machines and operations Capturing such a domain in a DBN would require exhaustively representing all possible objects and relations among them which is impractical Formalisms that can represent objects and relations as opposed to just variables have a long history in AI Recently signiﬁcant progress has been made in combining them with a principled treatment of uncertainty In particular probabilistic relational models or PRMs  are an extension of Bayesian networks that allows reasoning with classes objects and relations Recently we proposed dynamic probabilistic relational models   which combine PRMs and DBNs to allow reasoning with classes objects and relations in a dynamic environment We also developed a relational Rao Blackwellized particle ﬁltering mechanism for state monitoring in DPRMs c2005 In this paper we introduce relational dynamic Bayesian networks  which extend DBNs to ﬁrstorder  domains RDBNs subsume DPRMs and have several advantages over them including greater simplicity and expressivity Furthermore they may be more easily learned using ILP techniques We develop a series of efﬁcient inference procedures for RDBNs  The RaoBlackwellised particle ﬁltering described in our previous paper requires two strong assumptions which restrict its applicability We lift these assumptions developing two new forms of particle ﬁltering In the ﬁrst approach we build an abstraction hierarchy over the ﬁrstorder predicates and use it to smooth the particle ﬁlter estimates In the second approach we introduce a variant of kernel density estimation with a kernel function speciﬁcally designed for relational domains Early fault detection can greatly reduce the cost of manufacturing processes In this paper we apply our inference algorithms to executionmonitoring of assembly plans showing that our methods scale to problems with over a thousand objects and thousands of steps Other domains where our techniques may be helpful include robot control vision in motion language processing computational modeling of markets battleﬁeld management cell biology ecosystem modeling and analysis of Web information The following are the signiﬁcant contributions of this paper  We deﬁne relational dynamic Bayesian networks  which allow modeling uncer tainty in dynamic relational domains  We present several novel methods for inferencing in RDBNs which use RaoBlackwellization particle ﬁltering smoothing on relational abstraction hierarchies and relational kernel density estimation  We apply RDBNs to fault diagnosis in factory assembly processes showing that the inference algorithms we propose outperform traditional particle ﬁltering The rest of the paper is structured as follows In Section 2 we review DBNs and brieﬂy discuss the different ﬁltering algorithms applicable to them We introduce RDBNs in Section 3 and in Sections 4 5 and 6 we describe our inference methods In Section 7 we report our experimental results In Section 8 we discuss the related work and we show that RDBNs subsume DPRMs We conclude with a discussion of future work 2 Background A Bayesian network encodes the joint probability distribution of a set of variables Z1     Zd as a directed acyclic graph and a set of conditional probability models Each node corresponds to a variable and the model associated with it allows us to compute the probability of a state of the variable given the state of its parents The set of parents of Zi denoted Pa is the set of nodes with an arc to Zi in the graph\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Conclusion for Relational_Dynamic_Bayesian_Networks.pdf:\n",
      "Our algo rithms use the same amount of memory as PF but require additional time  to do smoothing Thus in our experiments the number of particles used by standard PF SPF and ASPF are 100000 50000 and 20000 respectively One can see that PF tends to diverge very quickly  while ASPF performs best and its approximation to the marginal distribution is close to the actual distribution Although the abstrac tion smoothing algorithm has low error we observe in the graph that the error increases with time We attribute this growth to the fact that the effective dimension of the assembly domain increases over time as new  relations are created making it increasingly difﬁcult to approx imate the distribution with a ﬁxed number of particles  shows the results of experiments for varying fault probabilities From the two ﬁgures we can conclude that the performance of the 784 RELATIONAL DYNAMIC BAYESIAN NETWORKS 0 KL Divergence Time Step 2540 2060 PF1 SPF1 ASPF1 PF10 SPF10 ASPF10  ASPF predicts the marginal distributions most accurately for varying fault probabilities 1 10 and 1000 objects standard PF degrades with increasing fault probability and with the number of objects while ASPF remains almost unaffected Next we report experiments when Assumption A1 holds and compare ASPS with RaoBlackwe llized particle ﬁltering The experiment was performed on 1000 objects with fault probability pf  1   shows the mean KL divergence between the approximate marginal distri butions and the true ones We can see that the difference between the KL divergence of ASPS and the KL divergence of RBPF is very small and this difference remains almost constant over time We conclude that the approximations underlying abstraction smoothing are quite good  shows that the KL divergence for ASPF is greater than RBPF by at most 001 and is on average around 0005 indicating that our approximations are quite good We also compare RBPF and ASPF when the number of relationships per object is increased  plots the KL divergence at the last step when the two algorithms are run on 1000 objects and 1 pf and varying number of objects per slot One can see that the difference is always less than 03 and the curve is quite stable 74 Computing Joint Distributions Figures 16 and 17 show the KL divergence of the full joint distribution of the state  for PF PF with abstraction smoothing  and PF using relational kernel density estimation  on experiments done with varying number of objects and fault probabilities respectively One can see that the estimates for the joint proba bility have greater KL divergence  and RKDE gives the best results From the experiments we conclude that RKDE can estimate the joint distribution quite accurately 785 SANGHAI DOMINGOS  WELD 0 The following are the conclusions that we can draw from the experiments  All of our algorithms  are much more accurate than standard PF for inference in RDBNs using similar computational resources  RBPF is the best method when Assumption A1 holds and scales up to a small maximum number of relations per object per predicate  PF with abstraction smoothing unlike RBPF is applicable in all scenarios to estimate the marginal distributions and is quite accurate  For estimating joint distributions relational kernel density estimation outperforms PF with abstraction smoothing 8 Related Work In recent years much research has focused on extending Bayesian networks to domains with rela tional structure Approaches include stochastic logic programs  probabilistic relational models Friedman et al 1999 The main difference is that we specify the probabilistic dependencies using FOPTs whereas Jaeger uses the notion of combination functions such as noisyor and equality constraints to deﬁne probability formulae over multisets However there has been very limited work on extending these to temporal domains Dynamic objectoriented Bayesian networks   combine DBNs with OOBNs a predecessor of PRMs Unfortunately no efﬁcient inference methods were proposed for DOOBNs and they have not been evaluated experimentally Glesner and Koller  proposed the idea of adding the power of ﬁrstorder logic to DBNs However they only give procedures for constructing ﬂexible DBNs out of ﬁrstorder knowledge bases and do not consider inference or learning procedures Like DOOBNs these were also not evaluated experimentally Relational Markov models  Anderson et al 2002 and logical hidden Markov models   are an extension of HMMs to ﬁrstorder domains In our previous work we introduced dynamic probabilistic relational domains  Sang hai et al 2003 which are an extension of PRMs DPRMs can be viewed as a combination of PRMs and dynamic Bayesian networks DPRMs are based on framebased systems which model the world in terms of classes objects and their attributes Objects are instances of classes and each class has a set of propositional attributes and relational attributes  The propositional attributes represent the properties of an object and the relational attributes model relationships be tween two objects A DPRM speciﬁes a probability distribution for each attribute of each class as a conditional probability table\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Enhanced Summary for Relational_Dynamic_Bayesian_Networks.pdf:\n",
      "Stochastic processes that involve the creation of objects and relations over time are widespread but relatively poorly studied. Dynamic Bayesian networks represent the state of the world as a set of variables and model the probabilistic dependencies of the variables within and between time steps. We apply our inference algorithms to executionmonitoring of assembly plans showing that our methods scale to problems with over a thousand objects and thousands of steps. Other domains where our techniques may be helpful include robot control vision in motion language processing computational modeling of markets battleﬁeld management ecosystem modeling and analysis of Web information. We present several methods for inferencing in dynamic dynamic dynamic Bayesian Networks which allow modeling in dynamic domains. We use the RaoBlackwellisation particle smoothing and kernel density estimation methods. We discuss the different algorithms applicable to relational RDBNs in Section 3 and introduce our inference methods in Section 4. In Section 7 we describe our experimental results in assembly processes showing that the inference algorithms we propose outperform traditional particle ﬁltering. The rest of the paper is structured as follows: In Section 2 we review DBNs and brie brie models and discuss the different algorithms applicable to them. In Sections 3 and 4 we describe the different inference methods and related results in Section 7 and 8 we discuss the experimental results.\n",
      "--------------------------------------------------------------------------------\n",
      "[DEBUG] Comparison for Relational_Dynamic_Bayesian_Networks.pdf:\n",
      "Relevance Score: 30.47% - Common words: 110 / 361\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for Constructing_Folksonomies_from_User_specified_Relations_on_Flickr.pdf:\n",
      "Many social Web sites allow users to publish content and annotate with descriptive metadata In addition to ﬂat tags some social Web sites have recently began to allow users to organize their content and metadata hierarchically The social photosharing site Flickr for example allows users to group related photos in sets and related sets in collec tions The social bookmarking site Delicious similarly lets users group related tags into bundles Although the sites themselves dont impose any constraints on how these hierarchies are used individuals generally use them to capture relationships between concepts most commonly the broadernarrower relations Collective annotation of content with hier archical relations may lead to an emergent classiﬁcation system called a folksonomy While some researchers have explored using tags as evidence for learning folksonomies we believe that hierarchical relations described above oﬀer a highquality source of evidence for this task We propose a simple approach to aggregate shallow hierarchies created by many distinct Flickr users into a common folksonomy Our approach uses statistics to determine if a particular relation should be retained or discarded The relations are then woven together into larger hierarchies Although we have not carried out a detailed quantitative evaluation of the approach it looks very promising since it generates very reasonable nontrivial hierarchies Key words Folksonomies Taxonomies Collective Knowledge Social Information Processing Data Mining 1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for Constructing_Folksonomies_from_User_specified_Relations_on_Flickr.pdf:\n",
      "Many social Web sites allow users to publish content and annotate with descriptive metadata In addition to ﬂat tags some social Web sites have recently began to allow users to organize their content and metadata hierarchically The social photosharing site Flickr for example allows users to group related photos in sets and related sets in collec tions The social bookmarking site Delicious similarly lets users group related tags into bundles Although the sites themselves dont impose any constraints on how these hierarchies are used individuals generally use them to capture relationships between concepts most commonly the broadernarrower relations Collective annotation of content with hier archical relations may lead to an emergent classiﬁcation system called a folksonomy While some researchers have explored using tags as evidence for learning folksonomies we believe that hierarchical relations described above oﬀer a highquality source of evidence for this task We propose a simple approach to aggregate shallow hierarchies created by many distinct Flickr users into a common folksonomy Our approach uses statistics to determine if a particular relation should be retained or discarded The relations are then woven together into larger hierarchies Although we have not carried out a detailed quantitative evaluation of the approach it looks very promising since it generates very reasonable nontrivial hierarchies Key words Folksonomies Taxonomies Collective Knowledge Social Information Processing Data Mining 1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Introduction for Constructing_Folksonomies_from_User_specified_Relations_on_Flickr.pdf:\n",
      "Although the sites themselves dont impose any constraints on how these hierarchies are used individuals generally use them to capture relationships between concepts most commonly the broadernarrower relations Collective annotation of content with hier archical relations may lead to an emergent classiﬁcation system called a folksonomy While some researchers have explored using tags as evidence for learning folksonomies we believe that hierarchical relations described above oﬀer a highquality source of evidence for this task We propose a simple approach to aggregate shallow hierarchies created by many distinct Flickr users into a common folksonomy Our approach uses statistics to determine if a particular relation should be retained or discarded The relations are then woven together into larger hierarchies Although we have not carried out a detailed quantitative evaluation of the approach it looks very promising since it generates very reasonable nontrivial hierarchies Key words Folksonomies Taxonomies Collective Knowledge Social Information Processing Data Mining 1 Introduction The subject of automatic taxonomy creation has attracted much attention from the academic community because of its close ties to important topics in philoso phy cognitive and computer sciences and information technology A taxonomy is a classiﬁcation system that helps people organize their knowledge of the world hierarchically through broadernarrower superclasssubclass relations between concepts One of the best known taxonomies is the Linnean classiﬁcation of living organisms There are alternative classiﬁcation systems for organizing knowledge that do not rely exclusively on strict hierarchies These include faceted classi ﬁcation schemes which combine multiple taxonomies to represent objects the arXiv08053747v1 csAI 24 May 2008 2 Anon Plangprasopchok and Kristina Lerman various library classiﬁcation schemes such as the Dewey Decimal system and Web directories eg Yahoo directory and the Open Directory Project which were created to categorize Web pages Despite variations in structure formal classiﬁcation systems are distinguished by the fact that they use a controlled vocabulary and are created by a small group of experts This means that formal classiﬁcations systems are often expensive to create and use and it is diﬃcult to keep them current in a fastchanging environment Take for example Web directories However because Web changes at a rapid pace with new pages added constantly and content of ex isting pages changing it was diﬃcult to keep the directory current The Open Directory Project  attempted to mitigate some of these concerns by al lowing a community of volunteers to edit a common Web directory Although any user can register to become an editor she still has to learn the structure and vocabulary and abide by the rules of the ODP As social Web sites such as Flickr Delicious and YouTube become in creasingly popular massive amount of metadata about the content created by users is now available The metadata comes in a variety of forms including tags the freelychosen keywords used to describe content as well as links users create between content metadata and other users Although users annotate content for personal use usergenerated metadata can be used to discover relevant re sources  personalize search  and automatically generate taxonomies 35 Such a bottomup taxonomy  a folksonomy  has a number of advantages over formal topdown classiﬁcation systems  it is dynamic evolving in time as communitys needs and vocabulary change  describes facets of data that are salient to users and  has a level of detail that is meaningful to users Sim ilar to a formal classiﬁcation system an automatically generated folksonomy could be used for information management and discovery as well as to annotate usergenerated content in order to make it machinereadable The current approaches to automatic folksonomy creation combine tags cre ated by large numbers of distinct individuals by looking at statistics of their occurrence In this paper we propose a simple framework for aggregating shallow indi vidual hierarchies into a common folksonomy1 We use the shallow hierarchies created through the collectionset relations on Flickr Second we propose a simple statistical approach to resolve hierarchical relation conﬂicts in the aggregation process Al though we dont undertake a quantitative evaluation of the learner folksonomies they appear to be very reasonable and detailed 2 Related work Many researchers have studied the problem of extracting ontological relations from text eg 68 These works exploit linguistic patterns to infer if two keywords are related under a certain relationship For instance they use such as vehicles such as cars to learn hyponym relations Cimiano et al  also applies linguistic patterns to extract object properties and then uses Formal Concept Analysis  to infer conceptual hierarchies In FCA a given object consists of a set of attributes and some attributes are common to a subset of objects A concept A subsumes concept B if all objects in B  are also in\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Conclusion for Constructing_Folksonomies_from_User_specified_Relations_on_Flickr.pdf:\n",
      "The social Web sites allow users to contribute content and also provide tools to help them manage content by annotating it with descriptive tags and more recently with semantic relations By making large amount of such metadata available social Web sites enable researchers to empirically study how humans organize knowledge and also to learn a common classiﬁcation system a folkson omy from the data This paper describes a statistical approach to aggregating large number of simple broadernarrower relations speciﬁed by diﬀerent users into a common deeper folksonomy The broadernarrow relations we used for Constructing Folksonomies from Userspeciﬁed Relations on Flickr 13 Fig 6 Concept graph associated with the concept invertebrate showing its broader and related narrower concepts the study were expressed through the shallow hierarchies of photo sets and col lections created by Flickr users to manage their photos Our approach is general and can be applied to other systems that allow users to specify relations eg the social bookmarking site Delicious allows users to group related tags into tag bundles Our longterm goal is to learn the structure of collective knowledge from the evidence provided by many users  We believe that the simple relations described above are more informative than tags alone for learning how people classify things Although we have not quantitatively compared the folksonomy learned by our approach to existing classiﬁcation systems qualitative evaluation indicates that our baseline method already yields good quality folksonomies There is still much room for improvement In the future we plan to separate broadernarrower from relatedto relations We also need to more system atically handle the challenges of diﬀerent users using a diﬀerent classiﬁcation order and diﬀerent level of speciﬁcity in the relations they specify We would also like to combine relations with tag statistics to disambiguate concepts We would also like to perform a systematic evaluation of the learned folksonomies eg by comparing learned structures to ODPs dmoz the open Web directory 14 Anon Plangprasopchok and Kristina Lerman Acknowledgements We would like to thank Fetch Technologies for providing us with a Web page scraping tool We also appreciate yWorks for providing yEd freely available for visualizing concept relations\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Enhanced Summary for Constructing_Folksonomies_from_User_specified_Relations_on_Flickr.pdf:\n",
      "We propose a simple approach to aggregate shallow hierarchies created by many distinct Flickr users into a common folksonomy. Our approach uses statistics to determine if a particular relation should be retained or discarded The relations are then woven together into larger hierarchies Although we have not carried out a detailed quantitative evaluation of the approach it looks very promising since it generates very reasonable nontrivial hierarchies. By making large amount of metadata available social Web sites enable researchers to study how humans organize knowledge and learn how to learn folksonomies. The Web sites allow users to contribute to the study of human knowledge and also contribute to research into how people organize their knowledge of the world. The current approaches to automatic folksonomy creation combine tags cre ated by large numbers of distinct individuals by looking at statistics of their occurrence. In this paper we propose asimple framework for aggregating shallow indi vidual hierarchies into acommon folksonomy1 We use the shallow hierarchy created through the collectionset relations on Flickr Second we propose  a simple statistical approach to resolve hierarchical relation conﬂicts in the aggregation process Al though we dont undertake a quantitative evaluation  they appear to be very reasonable and detailed and detailed 2 Related works exploit linguistic patterns to extract ontological relations from text eg 68 These works exploited linguistic patterns  to extract object properties and extract object relations.\n",
      "--------------------------------------------------------------------------------\n",
      "[DEBUG] Comparison for Constructing_Folksonomies_from_User_specified_Relations_on_Flickr.pdf:\n",
      "Relevance Score: 51.01% - Common words: 76 / 149\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for Learning_Concept_Hierarchies_from_Text_Corpora_using_Formal_Concept___Analysis.pdf:\n",
      "We present a novel approach to the automatic acquisition of taxonomies or concept hierarchies from a text corpus The approach is based on Formal Concept Analysis  a method mainly used for the analysis of data ie for investigating and processing explicitly given information We follow Harris distributional hypothesis and model the context of a certain term as a vector repre senting syntactic dependencies which are automatically acquired from the text corpus with a lin guistic parser On the basis of this context information FCA produces a lattice that we convert into a special kind of partial order constituting a concept hierarchy The approach is evaluated by com paring the resulting concept hierarchies with handcrafted taxonomies for two domains tourism and ﬁnance We also directly compare our approach with hierarchical agglomerative clustering as well as with BiSectionKMeans as an instance of a divisive clustering algorithm However it is also well known that any knowledgebased system suffers from the socalled knowledge acquisition bottleneck ie the difﬁculty to actually model the domain in question In c 2005 AI Access Foundation All rights reserved CIMIANO HOTHO  STAAB order to partially overcome this problem we present a novel approach to automatically learning a concept hierarchy from a text corpus Making the knowledge implicitly contained in texts explicit is a great challenge For example Brewster Ciravegna and Wilks  have argued that text writing and reading is in fact a process of background knowledge maintenance in the sense that basic domain knowledge is assumed and only the relevant part of knowledge which is the issue of the text or article is mentioned in a more or less explicit way Actually knowledge can be found in texts at different levels of explicitness depending on the sort of text considered Handbooks textbooks or dictionaries for example contain explicit knowledge in form of deﬁnitions such as a tiger is a mammal or mammals such as tigers lions or elephants In fact some researchers have exploited such regular patterns to discover taxonomic or partof relations in texts  However it seems that the more technical and specialized the texts get the less basic knowledge we ﬁnd stated explicitly Thus an interesting alternative is to derive knowledge from texts by analyzing how certain terms are used rather than to look for their explicit deﬁnition In these lines the distributional hypothesis  assumes that terms are similar to the extent to which they share similar linguistic contexts In fact different methods have been proposed in the literature to address the problem of semi automatically deriving a concept hierarchy from text based on the distributional hypothesis Basi cally these methods can be grouped into two classes the similaritybased methods on the one hand and the settheoretical on the other hand Both methods adopt a vectorspace model and represent a word or term as a vector containing features or attributes derived from a certain corpus There is certainly a great divergence in which attributes are used for this purpose but typically some sort of syntactic features are used such as conjunctions appositions  or verbargument dependencies Hindle 1990 Pereira Tishby  Lee 1993 Grefenstette 1994 Faure  Nedellec 1998 The ﬁrst type of methods is characterized by the use of a similarity or distance measure in order to compute the pairwise similarity or distance between vectors corresponding to two words or terms in order to decide if they can be clustered or not Some prominent examples for this type of method have been developed by Hindle  Pereira et al  Grefenstette  Faure and Nedellec  Caraballo  as well as Bisson Nedellec and Canamero  Set theoretical approaches partially order the objects according to the inclusion relations between their attribute sets\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for Learning_Concept_Hierarchies_from_Text_Corpora_using_Formal_Concept___Analysis.pdf:\n",
      "We present a novel approach to the automatic acquisition of taxonomies or concept hierarchies from a text corpus The approach is based on Formal Concept Analysis  a method mainly used for the analysis of data ie for investigating and processing explicitly given information We follow Harris distributional hypothesis and model the context of a certain term as a vector repre senting syntactic dependencies which are automatically acquired from the text corpus with a lin guistic parser On the basis of this context information FCA produces a lattice that we convert into a special kind of partial order constituting a concept hierarchy The approach is evaluated by com paring the resulting concept hierarchies with handcrafted taxonomies for two domains tourism and ﬁnance We also directly compare our approach with hierarchical agglomerative clustering as well as with BiSectionKMeans as an instance of a divisive clustering algorithm However it is also well known that any knowledgebased system suffers from the socalled knowledge acquisition bottleneck ie the difﬁculty to actually model the domain in question In c 2005 AI Access Foundation All rights reserved CIMIANO HOTHO  STAAB order to partially overcome this problem we present a novel approach to automatically learning a concept hierarchy from a text corpus Making the knowledge implicitly contained in texts explicit is a great challenge For example Brewster Ciravegna and Wilks  have argued that text writing and reading is in fact a process of background knowledge maintenance in the sense that basic domain knowledge is assumed and only the relevant part of knowledge which is the issue of the text or article is mentioned in a more or less explicit way Actually knowledge can be found in texts at different levels of explicitness depending on the sort of text considered Handbooks textbooks or dictionaries for example contain explicit knowledge in form of deﬁnitions such as a tiger is a mammal or mammals such as tigers lions or elephants In fact some researchers have exploited such regular patterns to discover taxonomic or partof relations in texts  However it seems that the more technical and specialized the texts get the less basic knowledge we ﬁnd stated explicitly Thus an interesting alternative is to derive knowledge from texts by analyzing how certain terms are used rather than to look for their explicit deﬁnition In these lines the distributional hypothesis  assumes that terms are similar to the extent to which they share similar linguistic contexts In fact different methods have been proposed in the literature to address the problem of semi automatically deriving a concept hierarchy from text based on the distributional hypothesis Basi cally these methods can be grouped into two classes the similaritybased methods on the one hand and the settheoretical on the other hand Both methods adopt a vectorspace model and represent a word or term as a vector containing features or attributes derived from a certain corpus There is certainly a great divergence in which attributes are used for this purpose but typically some sort of syntactic features are used such as conjunctions appositions  or verbargument dependencies Hindle 1990 Pereira Tishby  Lee 1993 Grefenstette 1994 Faure  Nedellec 1998 The ﬁrst type of methods is characterized by the use of a similarity or distance measure in order to compute the pairwise similarity or distance between vectors corresponding to two words or terms in order to decide if they can be clustered or not Some prominent examples for this type of method have been developed by Hindle  Pereira et al  Grefenstette  Faure and Nedellec  Caraballo  as well as Bisson Nedellec and Canamero  Set theoretical approaches partially order the objects according to the inclusion relations between their attribute sets\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Introduction for Learning_Concept_Hierarchies_from_Text_Corpora_using_Formal_Concept___Analysis.pdf:\n",
      "Abstract We present a novel approach to the automatic acquisition of taxonomies or concept hierarchies from a text corpus The approach is based on Formal Concept Analysis  a method mainly used for the analysis of data ie for investigating and processing explicitly given information We follow Harris distributional hypothesis and model the context of a certain term as a vector repre senting syntactic dependencies which are automatically acquired from the text corpus with a lin guistic parser On the basis of this context information FCA produces a lattice that we convert into a special kind of partial order constituting a concept hierarchy The approach is evaluated by com paring the resulting concept hierarchies with handcrafted taxonomies for two domains tourism and ﬁnance We also directly compare our approach with hierarchical agglomerative clustering as well as with BiSectionKMeans as an instance of a divisive clustering algorithm However it is also well known that any knowledgebased system suffers from the socalled knowledge acquisition bottleneck ie the difﬁculty to actually model the domain in question In c 2005 AI Access Foundation All rights reserved CIMIANO HOTHO  STAAB order to partially overcome this problem we present a novel approach to automatically learning a concept hierarchy from a text corpus Making the knowledge implicitly contained in texts explicit is a great challenge For example Brewster Ciravegna and Wilks  have argued that text writing and reading is in fact a process of background knowledge maintenance in the sense that basic domain knowledge is assumed and only the relevant part of knowledge which is the issue of the text or article is mentioned in a more or less explicit way Actually knowledge can be found in texts at different levels of explicitness depending on the sort of text considered Handbooks textbooks or dictionaries for example contain explicit knowledge in form of deﬁnitions such as a tiger is a mammal or mammals such as tigers lions or elephants In fact some researchers have exploited such regular patterns to discover taxonomic or partof relations in texts  However it seems that the more technical and specialized the texts get the less basic knowledge we ﬁnd stated explicitly Thus an interesting alternative is to derive knowledge from texts by analyzing how certain terms are used rather than to look for their explicit deﬁnition In these lines the distributional hypothesis  assumes that terms are similar to the extent to which they share similar linguistic contexts In fact different methods have been proposed in the literature to address the problem of semi automatically deriving a concept hierarchy from text based on the distributional hypothesis Basi cally these methods can be grouped into two classes the similaritybased methods on the one hand and the settheoretical on the other hand Both methods adopt a vectorspace model and represent a word or term as a vector containing features or attributes derived from a certain corpus There is certainly a great divergence in which attributes are used for this purpose but typically some sort of syntactic features are used such as conjunctions appositions  or verbargument dependencies Hindle 1990 Pereira Tishby  Lee 1993 Grefenstette 1994 Faure  Nedellec 1998 The ﬁrst type of methods is characterized by the use of a similarity or distance measure in order to compute the pairwise similarity or distance between vectors corresponding to two words or terms in order to decide if they can be clustered or not Some prominent examples for this type of method have been developed by Hindle  Pereira et al  Grefenstette  Faure and Nedellec  Caraballo  as well as Bisson Nedellec and Canamero  Set theoretical approaches partially order the objects according to the inclusion relations between their attribute sets\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Conclusion for Learning_Concept_Hierarchies_from_Text_Corpora_using_Formal_Concept___Analysis.pdf:\n",
      "on of the clusters created in each step The authors present the results of their system in terms of cluster accuracy in dependency of percentage of the corpus used Caraballo  also uses clustering methods to derive an unla beled hierarchy of nouns by using data about conjunctions of nouns and appositions collected from the Wall Street Journal corpus Interestingly in a second step she also labels the abstract concepts of the hierarchy by considering the Hearst patterns  in which the children of the concept in question appear as hyponyms The most frequent hypernym is then chosen in order to label the concept At a further step she also compresses the produced ontological tree by eliminating internal nodes without a label The ﬁnal ontological tree is then evaluated by presenting a random choice of clusters and the corresponding hypernym to three human judges for validation Bisson et al  present an interesting framework and a corresponding workbench  MoK  allowing users to design conceptual clustering methods to assist them in an ontology building task In particular they use bottomup clustering and compare different similarity measures as well as different pruning parameters In earlier work we used collocation statistics to learn relations between terms using a modi ﬁcation of the association rules extraction algorithm However these relations were not inherently taxonomic such that the work described in this paper can not be di 330 LEARNING CONCEPT HIERARCHIES FROM TEXT CORPORA USING FORMAL CONCEPT ANALYSIS rectly compared to it Maedche Pekar and Staab  examined different supervised techniques based on collocations to ﬁnd the appropriate hypernym for an unknown term reaching an accuracy of around 15 using a combination of a tree ascending algorithm and kNearestNeighbors as well as the Skew Divergence as similarity measure These results are neither comparable to the task at hand Recently Reinberger and Spyns  have presented an application of clustering tech niques in the biomedical domain They evaluate their clusters by directly comparing to the UMLS thesaurus Their results are very low 317 precision depending on the corpus and clustering tech nique and comparable to the results we obtained when comparing our clusters directly with our gold standards and which are not reported in this paper though Furthermore there is quite a lot of work related to the use of linguistic patterns to discover certain ontological relations from text Hearsts seminal approach aimed at discovering taxonomic relations from electronic dictionaries  The precision of the isarelations learned is 61106 5755 when measured against WordNet as gold standard Hearsts idea has been reapplied by different researchers with either slight variations in the patterns used Iwanska et al 2000 in very speciﬁc domains Ahmad et al 2003 to acquire knowledge for anaphora resolution  or to discover other kinds of semantic relations such as partof relations  or causation relations  The approaches of Hearst and others are characterized by a  high precision in the sense that the quality of the learned relations is very high However these approaches suffer from a very low recall which is due to the fact that the patterns are very rare As a possible solution to this problem in the approach of Cimiano Pivk SchmidtThieme and Staab  Hearst patterns matched in a corpus and on the Web as well as explicit information derived from other resources and heuristics are combined yielding better results compared to considering only one source of evidence on the task of learning superconcept relations In general to overcome such data sparseness problems researchers are more and more resorting to the WWW as for example Markert Modjeska and Nissim  In their approach Hearst patterns are searched for on the WWW by using the Google API in order to acquire background knowledge for anaphora resolution Agirre Ansa Hovy and Martinez  download related texts from the Web to enrich a given ontology Cimiano Handschuh and Staab  as well as Cimiano Ladwig and Staab  have used the Google API to match Hearstlike patterns on the Web in order to  ﬁnd the best concept for an unknown instance as well as  the appropriate superconcept for a certain concept in a given ontology  Velardi Fabriani and Missikoff  present the OntoLearn system which discovers i the domain concepts relevant for a certain domain ie the relevant terminology ii named entities iii vertical isa or taxonomic relations as well as iv certain relations between concepts based on speciﬁc syntactic relations In their approach a vertical relation is established between a term t 1 and a term t 2 ie isa if t 2 can be gained out of t 1 by stripping of the latters prenominal modiﬁers such as adjectives or modifying nouns Thus a vertical relation is for example estab lished between the term international credit card and the term credit card ie isa In a further paper  the main focus is on the task of word sense disambiguation ie of ﬁnding the correct sense of a word with respect to a general ontology or lexical database In particular they present a novel algorithm called SSI relying on the structure of the general ontology for this purpose\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Enhanced Summary for Learning_Concept_Hierarchies_from_Text_Corpora_using_Formal_Concept___Analysis.pdf:\n",
      "The approach is based on Formal Concept Analysis  a method mainly used for the analysis of data. We follow Harris distributional hypothesis and model the context of a certain term as a vector repre senting syntactic dependencies which are automatically acquired from the text corpus. The approach is evaluated by com paring the resulting concept hierarchies with handcrafted taxonomies for two domains tourism and ﬁnance. We also directly compare our approach with hierarchical agglomerative clustering as well as with BiSectionKMeans as an instance of a divisive clustering algorithm. The authors present the results of their system in terms of cluster accuracy in dependency of percentage of the corpus used. They also present an interesting framework and corresponding workbench allowing users to design conceptual hierarchies using MoKK. However these relations were not inherently taxonomic and can not be inherently extracted from a text corpus using the algorithm described in this paper. In particular they use bottom-up clustering methods to assist them in an ontology building task in order to learn taxonomic relations between terms using a modi-cation of the association rules of the taxonomic taxonomic extraction method. The most frequent hypernym is chosen to label the concept without eliminating the concept. At a further step she also compresses the produced ontological tree by presenting a random choice of clusters and correspondinghypernym to three human judges for validation.\n",
      "--------------------------------------------------------------------------------\n",
      "[DEBUG] Comparison for Learning_Concept_Hierarchies_from_Text_Corpora_using_Formal_Concept___Analysis.pdf:\n",
      "Relevance Score: 28.87% - Common words: 84 / 291\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for An_Expressive_Language_and_Efficient_Execution_System_for_Software___Agents.pdf:\n",
      "Software agents can be used to automate many of the tedious timeconsuming information processing tasks that humans currently have to complete manually However to do so agent plans must be capable of representing the myriad of actions and control flows required to perform those tasks In addition since these tasks can require integrating multiple sources of remote information  typically a slow IObound process  it is desirable to make execution as efficient as possible To address both of these needs we present a flexible software agent plan language and a highly parallel execution system that enable the efficient execution of expressive agent plans The plan language allows complex tasks to be more easily expressed by providing a variety of operators for flexibly processing the data as well as supporting subplans  and recursion The executor is based on a streaming dataflow model of execution to maximize the amount of operator and data parallelism possible at runtime We have implemented both the language and executor in a system called THESEUS Our results from testing THESEUS show that streaming dataflow execution can yield significant speedups over both traditional serial  as well as nonstreaming dataflowstyle execution that existing software and robot agent execution systems currently support In addition we show how plans written in the language we present can represent certain types of subtasks that cannot be accomplished using the languages supported by network query engines Finally we demonstrate that the increased expressivity of our plan language does not hamper performance specifically we show how data can be integrated from multiple remote sources just as efficiently using our architecture as is possible with a stateoftheart streamingdataflow network query engine 1 Introduction The goal of software agents is to automate tasks that require interacting with one or more accessible software systems Past research has yielded several types of agents and agent frameworks capable of automating a wide range of tasks including processing sequences of operating system commands  mediation of heterogeneous data sources  online comparison shopping  continual financial portfolio analysis  and airline ticket monitoring  to name only a few Despite software agent heterogeneity two recurring characteristics are  the wide variety of tasks that agents are used to automate and  the frequent need to process and route information during agent execution Perhaps no other domain poses as many tantalizing possibilities for software agent automation as the Web The ubiquity and practicality of the Web suggests that many potential benefits can be gained from automating tasks related to sources on the web Furthermore the Web is ripe for such automation  given the sheer number of online applications and the complete lack of coordination between them agents could address an endless list of needs and problems to be solved for people that do use the Web for practical purposes Furthermore like other software agent domains Web tasks vary widely in complexity and by definition involve routing and processing information as part of the task In this paper we describe a software agent plan language and execution system that enables one to express a wide range of tasks as a software agent plan and then to have that plan be efficiently executed We have implemented both the language and the executor in a system called THESEUS Throughout this paper we will discuss THESEUS in the context of Web information gathering and processing since the Web represents a domain where most  of the challenges that software agents face can be found 11 Web Information Agents In recent years the Web has experienced a rapid rate of growth with more and more useful information becoming available online Today there exists an enormous amount of online data that people can not only view but also use in order to accomplish real tasks Hundreds of thousands of people use the Web every day to research airfares monitor financial portfolios and keep up to date with the latest news headlines In addition to its enormity what is compelling about the Internet as a practical tool is its dynamic uptotheminute nature For example although information such as stock quotes and ticket availabilities change frequently many sources on the Internet are capable of reporting these updates immediately For this reason and because of the breadth and depth of information it provides the Web has become  for certain tasks  a more timely and necessary medium than even the daily newspaper radio or television The degree of complexity in gathering information from the Web varies significantly Some types of tasks can be accomplished manually because the size of the data gathered is small or the need to query is infrequent For example finding the address of a restaurant or a theater in a particular city using a Yellow Pages type of Web site is easy enough for people to do themselves It does not need to be automated since the query need only be done once and the result returned is small and easy to manage However not all information gathering tasks are as simple There are often times when the amount of data involved is large or the answer requires integrating data from multiple sites or the answer requires multiple queries over a period of time For example consider shopping for an expensive product over a period of time using multiple sources that are each updated daily Such tasks can become quickly tedious and require a greater amount of manual work making them very desirable to automate 111 MORE COMPLICATED TASKS One type of difficult Web information gathering task involves interleaved gathering and navigation For the benefit of people that use a Web browser to access online data many Web sources display large sets of query results spread over a series of web pages connected through Next Page links For example querying an online classified listings source for automobiles for sale can generate many results Instead of displaying the results on a single very long Web page many classified listings sites group sets of results over series of hyperlinked pages In order to automatically collect this data a system needs to interleave navigation and gathering an\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for An_Expressive_Language_and_Efficient_Execution_System_for_Software___Agents.pdf:\n",
      "Software agents can be used to automate many of the tedious timeconsuming information processing tasks that humans currently have to complete manually However to do so agent plans must be capable of representing the myriad of actions and control flows required to perform those tasks In addition since these tasks can require integrating multiple sources of remote information  typically a slow IObound process  it is desirable to make execution as efficient as possible To address both of these needs we present a flexible software agent plan language and a highly parallel execution system that enable the efficient execution of expressive agent plans The plan language allows complex tasks to be more easily expressed by providing a variety of operators for flexibly processing the data as well as supporting subplans  and recursion The executor is based on a streaming dataflow model of execution to maximize the amount of operator and data parallelism possible at runtime We have implemented both the language and executor in a system called THESEUS Our results from testing THESEUS show that streaming dataflow execution can yield significant speedups over both traditional serial  as well as nonstreaming dataflowstyle execution that existing software and robot agent execution systems currently support In addition we show how plans written in the language we present can represent certain types of subtasks that cannot be accomplished using the languages supported by network query engines Finally we demonstrate that the increased expressivity of our plan language does not hamper performance specifically we show how data can be integrated from multiple remote sources just as efficiently using our architecture as is possible with a stateoftheart streamingdataflow network query engine 1 Introduction The goal of software agents is to automate tasks that require interacting with one or more accessible software systems Past research has yielded several types of agents and agent frameworks capable of automating a wide range of tasks including processing sequences of operating system commands  mediation of heterogeneous data sources  online comparison shopping  continual financial portfolio analysis  and airline ticket monitoring  to name only a few Despite software agent heterogeneity two recurring characteristics are  the wide variety of tasks that agents are used to automate and  the frequent need to process and route information during agent execution Perhaps no other domain poses as many tantalizing possibilities for software agent automation as the Web The ubiquity and practicality of the Web suggests that many potential benefits can be gained from automating tasks related to sources on the web Furthermore the Web is ripe for such automation  given the sheer number of online applications and the complete lack of coordination between them agents could address an endless list of needs and problems to be solved for people that do use the Web for practical purposes Furthermore like other software agent domains Web tasks vary widely in complexity and by definition involve routing and processing information as part of the task In this paper we describe a software agent plan language and execution system that enables one to express a wide range of tasks as a software agent plan and then to have that plan be efficiently executed We have implemented both the language and the executor in a system called THESEUS Throughout this paper we will discuss THESEUS in the context of Web information gathering and processing since the Web represents a domain where most  of the challenges that software agents face can be found 11 Web Information Agents In recent years the Web has experienced a rapid rate of growth with more and more useful information becoming available online Today there exists an enormous amount of online data that people can not only view but also use in order to accomplish real tasks Hundreds of thousands of people use the Web every day to research airfares monitor financial portfolios and keep up to date with the latest news headlines In addition to its enormity what is compelling about the Internet as a practical tool is its dynamic uptotheminute nature For example although information such as stock quotes and ticket availabilities change frequently many sources on the Internet are capable of reporting these updates immediately For this reason and because of the breadth and depth of information it provides the Web has become  for certain tasks  a more timely and necessary medium than even the daily newspaper radio or television The degree of complexity in gathering information from the Web varies significantly Some types of tasks can be accomplished manually because the size of the data gathered is small or the need to query is infrequent For example finding the address of a restaurant or a theater in a particular city using a Yellow Pages type of Web site is easy enough for people to do themselves It does not need to be automated since the query need only be done once and the result returned is small and easy to manage However not all information gathering tasks are as simple There are often times when the amount of data involved is large or the answer requires integrating data from multiple sites or the answer requires multiple queries over a period of time For example consider shopping for an expensive product over a period of time using multiple sources that are each updated daily Such tasks can become quickly tedious and require a greater amount of manual work making them very desirable to automate 111 MORE COMPLICATED TASKS One type of difficult Web information gathering task involves interleaved gathering and navigation For the benefit of people that use a Web browser to access online data many Web sources display large sets of query results spread over a series of web pages connected through Next Page links For example querying an online classified listings source for automobiles for sale can generate many results Instead of displaying the results on a single very long Web page many classified listings sites group sets of results over series of hyperlinked pages In order to automatically collect this data a system needs to interleave navigation and gathering an\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Introduction for An_Expressive_Language_and_Efficient_Execution_System_for_Software___Agents.pdf:\n",
      "Greg Barish gbarishfetchcom Fetch Technologies 2041 Rosecrans Avenue Suite 245 El Segundo CA 90245 USA Craig A Knoblock knoblockisiedu University of Southern California Information Sciences Institute 4676 Admiralty Way Marina del Rey CA 90292 USA Abstract Software agents can be used to automate many of the tedious timeconsuming information processing tasks that humans currently have to complete manually However to do so agent plans must be capable of representing the myriad of actions and control flows required to perform those tasks In addition since these tasks can require integrating multiple sources of remote information  typically a slow IObound process  it is desirable to make execution as efficient as possible To address both of these needs we present a flexible software agent plan language and a highly parallel execution system that enable the efficient execution of expressive agent plans The plan language allows complex tasks to be more easily expressed by providing a variety of operators for flexibly processing the data as well as supporting subplans  and recursion The executor is based on a streaming dataflow model of execution to maximize the amount of operator and data parallelism possible at runtime We have implemented both the language and executor in a system called THESEUS Our results from testing THESEUS show that streaming dataflow execution can yield significant speedups over both traditional serial  as well as nonstreaming dataflowstyle execution that existing software and robot agent execution systems currently support In addition we show how plans written in the language we present can represent certain types of subtasks that cannot be accomplished using the languages supported by network query engines Finally we demonstrate that the increased expressivity of our plan language does not hamper performance specifically we show how data can be integrated from multiple remote sources just as efficiently using our architecture as is possible with a stateoftheart streamingdataflow network query engine 1 Introduction The goal of software agents is to automate tasks that require interacting with one or more accessible software systems Past research has yielded several types of agents and agent frameworks capable of automating a wide range of tasks including processing sequences of operating system commands  mediation of heterogeneous data sources  online comparison shopping  continual financial portfolio analysis  and airline ticket monitoring  to name only a few Despite software agent heterogeneity two recurring characteristics are  the wide variety of tasks that agents are used to automate and  the frequent need to process and route information during agent execution Perhaps no other domain poses as many tantalizing possibilities for software agent automation as the Web The ubiquity and practicality of the Web suggests that many potential benefits can be gained from automating tasks related to sources on the web Furthermore the Web is ripe for such automation  given the sheer number of online applications and the complete lack of coordination between them agents could address an endless list of needs and problems to be solved for people that do use the Web for practical purposes Furthermore like other software agent domains Web tasks vary widely in complexity and by definition involve routing and processing information as part of the task In this paper we describe a software agent plan language and execution system that enables one to express a wide range of tasks as a software agent plan and then to have that plan be efficiently executed We have implemented both the language and the executor in a system called THESEUS Throughout this paper we will discuss THESEUS in the context of Web information gathering and processing since the Web represents a domain where most  of the challenges that software agents face can be found 11 Web Information Agents In recent years the Web has experienced a rapid rate of growth with more and more useful information becoming available online Today there exists an enormous amount of online data that people can not only view but also use in order to accomplish real tasks Hundreds of thousands of people use the Web every day to research airfares monitor financial portfolios and keep up to date with the latest news headlines In addition to its enormity what is compelling about the Internet as a practical tool is its dynamic uptotheminute nature For example although information such as stock quotes and ticket availabilities change frequently many sources on the Internet are capable of reporting these updates immediately For this reason and because of the breadth and depth of information it provides the Web has become  for certain tasks  a more timely and necessary medium than even the daily newspaper radio or television The degree of complexity in gathering information from the Web varies significantly Some types of tasks can be accomplished manually because the size of the data gathered is small or the need to query is infrequent For example finding the address of a restaurant or a theater in a particular city using a Yellow Pages type of Web site is easy enough for people to do themselves It does not need to be automated since the query need only be done once and the result returned is small and easy to manage However not all information gathering tasks are as simple There are often times when the amount of data involved is large or the answer requires integrating data from multiple sites or the answer requires multiple queries over a period of time For example consider shopping for an expensive product over a period of time using multiple sources that are each updated daily Such tasks can become quickly tedious and require a greater amount of manual work making them very desirable to automate 111 MORE COMPLICATED TASKS One type of difficult Web information gathering task involves interleaved gathering and navigation For the benefit of people that use a Web browser to access online data many Web sources display large sets of query results spread over a series of web pages connected through Next Page links For example querying an online classified listings source for automobiles for sale can generate many results Instead of displaying the results on a single very long Web page many classified listings sites group sets of results over series of hyperlinked pages In order to automatically collect this data a system needs to interleave navigation and gathering an AN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS 627 indeterminate number of times that is it needs to collect results from a given page navigate to the next gather the next set of results navigate and so on until it reaches the end of set of results While there has been some work addressing how to theoretically incorporate navigation into the gathering process no attention has been given to the efficient execution of plans that engage in this type of interleaved retrieval A second example has to do with monitoring a Web source Since the Web does not contain a builtin trigger facility one is forced to manually check sources for updated data When updates are frequent or the need to identify an update immediately is urgent it becomes desirable to automate the monitoring of these updates notifying the user when one or more conditions are met For example suppose we want to be alerted as soon as a particular type of used car is listed for sale by one or more online classified ad sources Repeated manual checking for such changes is obviously tedious Mediators and network query engines can automate the query but additional software in programming languages such as Java or C must be written to handle the monitoring process itself something that requires conditional execution comparison with past results possible notification of the user and other such actions 112 THE NEED FOR FLEXIBILITY These examples show that automatically querying and processing Web data can involve a number of subtasks such as interleaved navigation and gathering and integration with local databases Because of these needs traditional database query languages like SQL are insufficient for the Web The root of the problem is lack of flexibility or expressivity in these languages  typically only querying is supported More complicated types of Web information gathering tasks such as those described here and in other articles Etzioni  Weld 1994 Doorenbos et al 1997 Chalupsky Gil Knoblock Lerman Oh Pynadath Russ  Tambe 2001 Ambite Barish Knoblock Muslea Oh  Minton 2002 Sycara Paolucci van Velsen  Giampapa 2003 Graham Decker  Mersic 2003 usually involve actions beyond those needed for merely querying ie beyond filtering and combining  they require plans capable of a variety of actions such as conditional execution integration with local databases and asynchronous notification to users In short Web information gathering tasks require an expressive query or plan language with which to describe a solution XQuery  used for querying XML documents is one language that offers more flexibility For example XQuery supports FLWOR expressions that allow one to easily specify how to iterate over data XQuery also supports conditional expressions UDFs and recursive functions Support for expressive agent plans can also be found in a number of software agent and robot agent frameworks such as INFOSLEUTH Bayardo et al 1997 RETSINA Sycara et al 2003 DECAF Graham et al 2003 RAPs  or PRSLITE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Conclusion for An_Expressive_Language_and_Efficient_Execution_System_for_Software___Agents.pdf:\n",
      "Though we have demonstrated that THESEUS performs well on more complex information gathering tasks it is useful to assess whether the increased expressivity in THESEUS impacts its performance on simpler tasks  in particular ones that network query engines typically process To do this we explored the performance of THESEUS on a more traditional database style query plan for online information gathering and compared it to the same type of plan executed by a network query engine We chose a single common type of SPJ query that involved multiple data sources to serve as the basis for comparison This is the canonical data integration query We claim that understanding how THESEUS compares to a network query engine with respect to the performance of an SPJ query is at the heart of the efficiency comparison between the two types of systems Since both types of systems execute dataflowstyle plans in pipelined fashion theoretical performance should be the same  the only expected differences would be due to implementation or environment biases eg different LAN infrastructures Nevertheless to support our efficiency claim we felt it was important to use a concrete SPJ query for comparison For our experiment we chose to reproduce a query from the paper of another network query engine To measure the performance of their partial results query processing technique Raman and Hellerstein ran a query that gathered data from three sources and then joined them together  The specific query involved gathering information on contributors to the 2000 US Presidential campaign and then combined this information with neighborhood demographic information and crime index information  lists the sources and the data they provide Bulk scannable sources are those where the data to be extracted can be read directly ie exists on a static Web page or file Index sources are those that provide answers based on queries via Web forms Index sources are thus sources which require binding patterns  shows the query that was used to evaluate the performance of TELEGRAPH It is important to note that Raman and Hellerstein measured the performance of the query in  under standard pipelined mode and compared this with their JuggleEddy partial results approach We are only interested in the results of the former as this is a measure of how well an unoptimized network query engine  what we call the baseline  gathers data when processing a traditional databasestyle query Any further optimization such as the JuggleEddy is complementary to the system described here Since both types of systems rely on streaming dataflow execution consisting of tuples routed through iterativestyle query operators it would not be difficult to extend THESEUS to support this and other types of adaptive query processing techniques Source Site Type of data FEC  Bulk scannable source that provides information  on each contributor to a candidate in the 2000 Presidential campaign Yahoo Real Estate realestsateyahoocom Index source that returns neighborhood demographic information for a particular zip code Crime  Index source that returns crime level ratings for a particular zip code  Sources used in the FECYahooCrime query BARISH  KNOBLOCK 656 We wrote a simple THESEUS plan that allowed the query in  to be executed We used exactly the same sources except we found that the latency of the Crime source had increased substantially as compared to the times recorded by Raman and Hellerstein Instead we used another source  but added an artificial delay to each tuple processed by that source so that the new source performed similarly Raman and Hellersteins results show that the performance of their pipeline plan was as slow as the Crime source and about 250ms per tuple To match this we added a 150ms delay to each tuple of processing for our new source Yahoo which was normally fetching data at about 100ms per tuple Our results are shown in  The results show that THESEUS was not only able to execute the same plan at least as fast as the baseline TELEGRAPH plan the nonoptimized result shown in  of the paper by Raman and Hellerstein but THESEUS execution can be more efficient depending on the number of threads in the thread pool For example THESEUS3 describes the case where the THESEUS thread pool contains 3 threads The result from this run performs slightly worse than the TELEGRAPH baseline  such minor differences could be due to changes in source behavior or in different proximities to network sources However running THESEUS with more threads in the thread pool ie THESEUS6 and THESEUS10 shows much better performance This is because the degree of vertical parallelism demanded during execution can be better accommodated with more threads It should be noted that the reason TELEGRAPH does not perform as well as THESEUS6 and THESEUS10 is likely because that system only assigned a single thread to each operator  That is THESEUS6 and THESEUS10 execution involves 6 and 10 concurrent threads respectively whereas the TELEGRAPH plan uses 3 concurrent threads 7 Related Work The language and system discussed in this paper are relevant to other efforts that focus on agent execution and the querying of Web data To understand the work presented here in the context of these other approaches we consider past work in software agent execution robot agent execution and network query engines AN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS 657 historically addressed expressivity issues and in recent years have also attempted to address some of the efficiency issues Robot plan executors represent a slightly greater contrast that have less experience with processing large amounts of data On the other hand network query engines have explored largescale remote data processing though planquery expressivity tends to be quite narrow 71 Software Agent Execution Systems The Internet Softbot  is a software agent that automates various information processing tasks including UNIX command processing and Web information gathering To support execution with incomplete information about the world the system interleaves planning with execution The XII Golden et al 1994 and later Puccini  planners generate partiallyordered plans in which the effects of an action do not have to be known before execution  but which can be verified during execution While the Softbot makes a clear distinction between information goals and satisfaction goals it does not specifically address the need to efficiently nor flexibly handle the information it processed For example the system does not support any kind of parallel processing of information to capitalize on the IObound nature of execution In terms of expressivity while XII and Puccini do allow universal quantification to be expressed ie iteration is possible to do so requires that the set of what is being iterated over be known in advance As we pointed out in an earlier example on Next Page links this is not always the case  the set of next pages to be processed are only discovered by iterating through all of them in an indeterminate dowhile fashion In contrast although it does not interleave planning and execution the system described here does support a more expressive plan language capable of handling nextlink type of processing as well as a streaming dataflow model of execution that enables efficient large scale information processing To a great extent contributions of both research efforts can be viewed as complementary Other research such as INFOSLEUTH Bayardo et al 1997 has recognized the importance of concurrent taskaction execution close to the spirit of true dataflow computing At the same time such work has generally not investigated the impact of streaming combined with dataflow INFOSLEUTH describes a collection of agents that when combined and working together present a cohesive view of data integration across multiple heterogeneous sources INFOSLEUTH centralizes execution in its Task Execution Agent which coordinates highlevel information gathering subtasks necessary to fulfill user queries by routing appropriate queries to resources that can accommodate those queries The Task Execution Agent is datadriven and thus task fulfillment proceeds in a dataflowstyle manner\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Enhanced Summary for An_Expressive_Language_and_Efficient_Execution_System_for_Software___Agents.pdf:\n",
      "Software agents can be used to automate many of the tedious timeconsuming information processing tasks that humans currently have to complete manually. To address both of these needs we present a flexible software agent plan language and a highly parallel execution system that enable the efficient execution of expressive agent plans. The plan language allows complex tasks to be more easily expressed by providing a variety of operators for flexibly processing the data. The executor is based on a streaming dataflow model of execution to maximize the amount of operator and data parallelism possible at runtime. We have implemented both the language and executor in a system called THESEUS. Our results from testing THESEUS show that streamingDataflow execution can yield significant speedups over both traditional serial  as well as nonstreaming dataflowstyle execution that existing software and robot agent execution systems currently support. We show how plans written in the language we present can represent certain types of subtasks that cannot be accomplished using the languages supported by network query engines. We demonstrate that the increased expressivity of our plan language does not hamper performance specifically we show how data can be integrated from multiple remote sources just as efficiently using our architecture as is possible with a stateoftheart streamingdataflow network query engine. We will discuss THESEUS in the context of Web information gathering and processing since the Web represents a domain where most  of the challenges that software agents face can be found.\n",
      "--------------------------------------------------------------------------------\n",
      "[DEBUG] Comparison for An_Expressive_Language_and_Efficient_Execution_System_for_Software___Agents.pdf:\n",
      "Relevance Score: 31.48% - Common words: 130 / 413\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for Integrating_Learning_from_Examples_into_the_Search_for_Diagnostic___Policies.pdf:\n",
      "This pap er studies the problem of learning diagnostic p olicies from training examples A diagnostic p olicy is a complete description of the decisionmaking actions of a diagnostician ie tests follo w ed b y a diagnostic decision for all p ossible com binations of test results An optimal diagnostic p olicy is one that minimizes the exp ected total cost whic h is the sum of measuremen t costs and misdiagnosis costs In most diagnostic settings there is a tradeo b et w een these t w o kinds of costs This pap er formalizes diagnostic decision making as a Mark o v Decision Pro cess  The pap er in tro duces a new family of systematic searc h algorithms based on the A O  algo rithm to solv e this MDP  T o The pap er then addresses the question of learning diagnostic p olicies from examples When the probabilities of diseases and test results are computed from training data there is a great danger of o v er tting T o reduce o v er tting regularizers are in tegrated in to the searc When measuremen t x n is executed the result is an observ ed v alue v n  F or example if x  is patien ts age then v  could b e   Eac h measuremen ose that in this starting kno wledge state the do ctor c ho oses measuremen t x  and receiv es the result that x    at a cost of 00 No w supp ose the do ctor c ho oses x   whic h measures b o dy mass index and receiv es a result x   smal l at a cost of  This c hanges the kno wledge state to fx    x   smal l g at a cost of C x     Finally  the do ctor mak es the diagnosis health y Supp ose that the correct diagnosis is y  diabetes F or illustrativ e purp oses  supp ose that the cost of this misdiagnosis is M C   00\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for Integrating_Learning_from_Examples_into_the_Search_for_Diagnostic___Policies.pdf:\n",
      "This pap er studies the problem of learning diagnostic p olicies from training examples A diagnostic p olicy is a complete description of the decisionmaking actions of a diagnostician ie tests follo w ed b y a diagnostic decision for all p ossible com binations of test results An optimal diagnostic p olicy is one that minimizes the exp ected total cost whic h is the sum of measuremen t costs and misdiagnosis costs In most diagnostic settings there is a tradeo b et w een these t w o kinds of costs This pap er formalizes diagnostic decision making as a Mark o v Decision Pro cess  The pap er in tro duces a new family of systematic searc h algorithms based on the A O  algo rithm to solv e this MDP  T o The pap er then addresses the question of learning diagnostic p olicies from examples When the probabilities of diseases and test results are computed from training data there is a great danger of o v er tting T o reduce o v er tting regularizers are in tegrated in to the searc When measuremen t x n is executed the result is an observ ed v alue v n  F or example if x  is patien ts age then v  could b e   Eac h measuremen ose that in this starting kno wledge state the do ctor c ho oses measuremen t x  and receiv es the result that x    at a cost of 00 No w supp ose the do ctor c ho oses x   whic h measures b o dy mass index and receiv es a result x   smal l at a cost of  This c hanges the kno wledge state to fx    x   smal l g at a cost of C x     Finally  the do ctor mak es the diagnosis health y Supp ose that the correct diagnosis is y  diabetes F or illustrativ e purp oses  supp ose that the cost of this misdiagnosis is M C   00\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Introduction for Integrating_Learning_from_Examples_into_the_Search_for_Diagnostic___Policies.pdf:\n",
      "This pap er studies the problem of learning diagnostic p olicies from training examples A diagnostic p olicy is a complete description of the decisionmaking actions of a diagnostician ie tests follo w ed b y a diagnostic decision for all p ossible com binations of test results An optimal diagnostic p olicy is one that minimizes the exp ected total cost whic h is the sum of measuremen t costs and misdiagnosis costs In most diagnostic settings there is a tradeo b et w een these t w o kinds of costs This pap er formalizes diagnostic decision making as a Mark o v Decision Pro cess  The pap er in tro duces a new family of systematic searc h algorithms based on the A O The pap er then addresses the question of learning diagnostic p olicies from examples When the probabilities of diseases and test results are computed from training data there is a great danger of o v er tting T o reduce o v er tting regularizers are in tegrated in to the searc When measuremen t x n is executed the result is an observ ed v alue v n  F or example if x  is patien ts age then v  could b e   Eac h measuremen ose that in this starting kno wledge state the do ctor c ho oses measuremen t x  and receiv es the result that x    at a cost of 00 No w supp ose the do ctor c ho oses x   whic h measures b o dy mass index and receiv es a result x   smal l at a cost of  This c hanges the kno wledge state to fx    x   smal l g at a cost of C x     Finally  the do ctor mak es the diagnosis health y Supp ose that the correct diagnosis is y  diabetes F or illustrativ e purp oses  supp ose that the cost of this misdiagnosis is M C   00\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Conclusion for Integrating_Learning_from_Examples_into_the_Search_for_Diagnostic___Policies.pdf:\n",
      "On breastcancer SPL is b etter than V OIL and again the di erence is sometimes signi can t In general the pair graphs con rm the c hess score results and supp ort our main conclusion that SPL is the most robust learning algorithm The results are plotted in Figure  The memory amoun ts plotted are computed b y taking the actual memory consumed b y our implemen tation and con v erting it to the memory that w ould b e consumed b y an optimized implemen tation There are sev eral imp ortan t conclusions to dra w from these gures First note that A O  without the admissible heuristic requires m uc h more memory than A O  with the admissible heuristic Hence the admissible heuristic is pruning large parts of the searc h space This is particularly eviden t at lo w settings of the misdiagnosis costs MC and MC A t these lo w settings A O  is able to nd man y cuto s b ecause the exp ected cost of diagnosis is less than the cost of making additional measuremen ts  The sa vings is m uc h smaller at MC lev els  and  The second imp ortan t conclusion is that the Laplace correction increases the size of the searc h space and the amoun t of memory consumed The reason is that without the Laplace correction man y test outcomes ha v e zero probabilit y A O   With the Laplace correction these outcomes m ust b e expanded and ev aluated The e ect is v ery minor at lo w MC lev els b ecause the ANDOR graph is m uc h smaller and consequen tly there is enough training data to prev en y outcomes But at high MC lev els the Laplace correction can cause increases of a factor of 0 or more in the amoun t of memory consumed The third imp ortan t conclusion is that statistical pruning signi can tly decreases the size of the ANDOR graph in almost all cases The only exception is heart at MC and MC where statistical pruning increases the amoun t of memory needed\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Enhanced Summary for Integrating_Learning_from_Examples_into_the_Search_for_Diagnostic___Policies.pdf:\n",
      "This pap er formalizes diagnostic decision making as a Mark o v Decision Pro cess. The pap er then addresses the question of learning diagnostic p olicies from examples. SPL is the most robust learning algorithm based on the A O. On breastcancer SPL is b etter than V OIL and again the di erence is sometimes signi can t. The Laplace correction increases the size of the searc h space and the amoun t of memory consumed. The third imp ortan t conclusion is that statistical pruning can tly decreases the size  of the ANDOR graph. The sa vings is m uc h smaller at MC lev els  and  and at MC and MC A t. In general the pair graphs con rm the c hess score results and supp ort our main conclusion that SPL is a robustlearning algorithm. The results are plotted in Figure  The memory amoun ts plotted are computed b y taking the actual memory consumed b y our implemen tation and con v erting it to the memory that w ould b e consumed b  y an optimized impleman tation. For example if x  is patien ts age then v  could b e    Eac h measuremen ose that in this starting kno wledge state  the correct diagnosis is y  diabetes.\n",
      "--------------------------------------------------------------------------------\n",
      "[DEBUG] Comparison for Integrating_Learning_from_Examples_into_the_Search_for_Diagnostic___Policies.pdf:\n",
      "Relevance Score: 41.33% - Common words: 62 / 150\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for Generalizing_Boolean_Satisfiability_II__Theory.pdf:\n",
      "This is the second of three planned papers describing zap a satisﬁability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high performance solvers The fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the Boolean representation used our goal is to deﬁne a representation in which this structure is apparent and can easily be exploited to improve computational performance This paper presents the theoretical basis for the ideas underlying zap arguing that existing ideas in this area exploit a single recurring structure in that multiple database axioms can be obtained by operating on a single axiom using a subgroup of the group of permutations on the literals in the problem We argue that the group structure precisely captures the general structure at which earlier approaches hinted and give numerous examples of its use We go on to extend the DavisPutnamLogemannLoveland inference procedure to this broader setting and show that earlier computational improvements are either subsumed or left intact by the new method The third paper in this series discusses zaps implementation and presents experimental performance results In the ﬁrst paper  to which we will refer as zap1 we discussed a variety of existing computational improvements to the 1 The ﬁrst paper has appeared in jair the third is currently available as a technical report  but has not yet been peer reviewed c2004 AI Access Foundation All rights reserved Dixon Ginsberg Luks  Parkes DavisPutnamLogemannLoveland  inference procedure eventually producing the following table The rows and columns are described on this page and the next eﬃciency proof propagation learning of repn length resolution technique method SAT  EEE  watched literals relevance cardinality exponential PE not unique watched literals relevance pseudo Boolean exponential PE unique watched literals  strengthening symmetry  EEE not believed in P same as sat same as sat QPROP exponential  in P using reasons exp improvement  ﬁrstorder The rows of the table correspond to observations regarding existing representations used in satisﬁability research as reﬂected in the labels in the ﬁrst column2 1 SAT refers to conventional Boolean satisﬁability work representing information as conjunctions of disjunctions of literals  2 cardinality refers to the use of counting clauses if we think of a conventional disjunction of literals ili as X i li 1 then a cardinality clause is one of the form\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for Generalizing_Boolean_Satisfiability_II__Theory.pdf:\n",
      "This is the second of three planned papers describing zap a satisﬁability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high performance solvers The fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the Boolean representation used our goal is to deﬁne a representation in which this structure is apparent and can easily be exploited to improve computational performance This paper presents the theoretical basis for the ideas underlying zap arguing that existing ideas in this area exploit a single recurring structure in that multiple database axioms can be obtained by operating on a single axiom using a subgroup of the group of permutations on the literals in the problem We argue that the group structure precisely captures the general structure at which earlier approaches hinted and give numerous examples of its use We go on to extend the DavisPutnamLogemannLoveland inference procedure to this broader setting and show that earlier computational improvements are either subsumed or left intact by the new method The third paper in this series discusses zaps implementation and presents experimental performance results In the ﬁrst paper  to which we will refer as zap1 we discussed a variety of existing computational improvements to the 1 The ﬁrst paper has appeared in jair the third is currently available as a technical report  but has not yet been peer reviewed c2004 AI Access Foundation All rights reserved Dixon Ginsberg Luks  Parkes DavisPutnamLogemannLoveland  inference procedure eventually producing the following table The rows and columns are described on this page and the next eﬃciency proof propagation learning of repn length resolution technique method SAT  EEE  watched literals relevance cardinality exponential PE not unique watched literals relevance pseudo Boolean exponential PE unique watched literals  strengthening symmetry  EEE not believed in P same as sat same as sat QPROP exponential  in P using reasons exp improvement  ﬁrstorder The rows of the table correspond to observations regarding existing representations used in satisﬁability research as reﬂected in the labels in the ﬁrst column2 1 SAT refers to conventional Boolean satisﬁability work representing information as conjunctions of disjunctions of literals  2 cardinality refers to the use of counting clauses if we think of a conventional disjunction of literals ili as X i li 1 then a cardinality clause is one of the form\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Introduction for Generalizing_Boolean_Satisfiability_II__Theory.pdf:\n",
      "On Time Systems Inc 1850 Millrace Suite 1 Eugene OR 97403 USA Matthew L Ginsberg ginsbergotsyscom On Time Systems Inc 1850 Millrace Suite 1 Eugene OR 97403 USA Eugene M Luks lukscsuoregonedu Computer and Information Science University of Oregon Eugene OR 97403 USA Andrew J Parkes parkescirluoregonedu CIRL 1269 University of Oregon Eugene OR 97403 USA Abstract This is the second of three planned papers describing zap a satisﬁability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high performance solvers The fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the Boolean representation used our goal is to deﬁne a representation in which this structure is apparent and can easily be exploited to improve computational performance This paper presents the theoretical basis for the ideas underlying zap arguing that existing ideas in this area exploit a single recurring structure in that multiple database axioms can be obtained by operating on a single axiom using a subgroup of the group of permutations on the literals in the problem We argue that the group structure precisely captures the general structure at which earlier approaches hinted and give numerous examples of its use We go on to extend the DavisPutnamLogemannLoveland inference procedure to this broader setting and show that earlier computational improvements are either subsumed or left intact by the new method The third paper in this series discusses zaps implementation and presents experimental performance results In the ﬁrst paper  to which we will refer as zap1 we discussed a variety of existing computational improvements to the 1 The ﬁrst paper has appeared in jair the third is currently available as a technical report  but has not yet been peer reviewed c2004 AI Access Foundation All rights reserved Dixon Ginsberg Luks  Parkes DavisPutnamLogemannLoveland  inference procedure eventually producing the following table The rows and columns are described on this page and the next eﬃciency proof propagation learning of repn length resolution technique method SAT  EEE  watched literals relevance cardinality exponential PE not unique watched literals relevance pseudo Boolean exponential PE unique watched literals  strengthening symmetry  EEE not believed in P same as sat same as sat QPROP exponential  in P using reasons exp improvement  ﬁrstorder The rows of the table correspond to observations regarding existing representations used in satisﬁability research as reﬂected in the labels in the ﬁrst column2 1 SAT refers to conventional Boolean satisﬁability work representing information as conjunctions of disjunctions of literals  2 cardinality refers to the use of counting clauses if we think of a conventional disjunction of literals ili as X i li 1 then a cardinality clause is one of the form Each wi is a positive integer giving the weight to be assigned to the associated literal 4 symmetry involves the introduction of techniques that are designed to explicitly exploit local or global symmetries in the problem being solved 5 QPROP deals with universally quantiﬁed formulae where all of the quantiﬁcations are over ﬁnite domains of known size The columns in the table measure the performance of the various systems against a variety of metrics 2 Please see the preceding paper zap1 Dixon et al 2004b for a fuller explanation and for a relatively comprehensive list of references where the earlier work is discussed 482 ZAP 2 Theory 1 Eﬃciency of representation measures the extent to which a single axiom in a proposed framework can replace many in cnf For cardinality pseudoBoolean and quantiﬁed languages it is possible that exponential savings are achieved We argued that such savings were possible but relatively unlikely for cardinality and pseudo Boolean encodings but were relatively likely for qprop 2 Proof length gives the minimum proof length for the representation on three classes of problems the pigeonhole problem parity problems due to Tseitin  and clique coloring problems An E indicates exponential proof length P indicates polynomial length While symmetryexploitation techniques can provide polynomiallength proofs in certain instances the method is so brittle against changes in the axiomatization that we do not regard this as a polynomial approach in general 3\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Conclusion for Generalizing_Boolean_Satisfiability_II__Theory.pdf:\n",
      "If G is a ﬁnite group and S G then S divides G 516 ZAP 2 Theory Proposition 43 Let S be a set of ground clauses and  an equivalent augmented clause where G is represented by generators It is possible in polynomial time to ﬁnd a set of generators ω1     ωk where k log2 G and G  ω1     ωk Proof Even the simplest approach suﬃces If G  gi checking to see if gi ω1     ωj for each generator gi can be performed in polynomial time using a wellknown method of Sims Dixon et al 2004a Deﬁnition A2 Let G Sym We will say that G acts transitively on S if for any x y S there is a g G with xg  y Proposition 48 Let  be an augmented clause with d distinct instances Then there is a subgroup H G that can be described using Olog2 generators such that    Furthermore given d and generators for G there is a Monte Carlo polynomialtime algorithm for constructing the generators of such an H Proof The basic ideas in the proof follow methods introduced by Babai Luks and Ser ess The proof of this particular result is a bit more involved than the others in this paper and following it is likely to require an existing familiarity with group theory Let D be the set of instances of  so that G acts transitively on D Now consider a sequence g1 g2    of uniformly distributed random elements of G and for each r 0 let Hr  g1 g2     grin particular H0 Wn resolveω ω  ωresolve Proof Suppose that the literal being resolved on is l so that if we think of c1 and c2 as being represented simply by the literals they contain the resolvent corresponds to c1 c2 l l Permuting with ω gives us ω The  case is similar so resolve G1 G2 is the canonical resolvent of  and  Proposition 63 Let p and q be quantiﬁed clauses such that there is a term tp in p and tq in q where tp and tq have common instances Suppose also that  is an augmented clause equivalent to p and  is an augmented clause equivalent to q where pg and qg resolve Then if no terms in p and q except for tp and tq have common instances  the result of resolving p and q in the conventional lifted sense is equivalent to resolve  Proof The proof is already contained in the discussion surrounding the example in the main text The base instance of the augmented resolvent is clearly an instance of the quantiﬁed resolution for the group we have already remarked that the group of stable extensions of the two embeddings corresponds simply to any bindings for the variables in the resolvents that can be extended to a permutation of all of the variables in question This means that the bindings must be consistent with regard to the values selected for shared terms and no two distinct quantiﬁed literals are mapped to identical ground atoms The latter condition follows from the assumption that the nonresolving literals have no common ground instances Proposition 64 There is an augmented resolution proof of polynomial size of the mutual unsatisﬁability of  and  Proof We begin by explaining how the proof goes generally and only subsequently provide the details From the fact that the ﬁrst pigeon has to be in one of the n holes we can conclude that one of the ﬁrst two pigeons must be in one of the last n 1 holes since these ﬁrst two pigeons cant both be in the ﬁrst hole Now one of the ﬁrst three pigeons must be in one of the last n 2 holes and so on until we conclude that one of the ﬁrst n pigeons must be in the last hole Similarly one of the ﬁrst n pigeons must be in each hole leaving no hole for the ﬁnal pigeon To formalize this we will write Ak for the fact that one of the ﬁrst k pigeons must be in one of the last n  1 k holes Ak  _\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Enhanced Summary for Generalizing_Boolean_Satisfiability_II__Theory.pdf:\n",
      "Zap is a satisﬁability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high performance solvers. The fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the Boolean representation used. We argue that the group structure precisely captures the general structure at which earlier approaches hinted and give numerous examples of its use. We go on to extend the DavisPutnamLogemannLoveland inference procedure to this broader setting and show that earlier computational improvements are either subsumed or left intact by the new method. The third paper in this series discusses zaps implementation and presents experimental performance results. The ﬁrst paper has appeared in jair the third is currently available as a technical report  but has not yet been peer reviewed c2004 AI Access Foundation All rights reserved Dixon Ginsberg Luks, Andrew J Parkes, Matthew L Ginsberg, Matthew Ginsberg and Andrew Parkes are the authors of this paper. The rows and columns are described on this page and the next eﬃciency proof propagation learning of repn length resolution technique method SAT. The columns in the table measure the performance of the various systems against a variety of metrics 2 Please see the preceding paper zap1 Dixon et al 2004b for a fuller explanation and for a relatively comprehensive list of references where the earlier work is discussed.\n",
      "--------------------------------------------------------------------------------\n",
      "[DEBUG] Comparison for Generalizing_Boolean_Satisfiability_II__Theory.pdf:\n",
      "Relevance Score: 57.67% - Common words: 124 / 215\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for On_the_Practical_use_of_Variable_Elimination_in_Constraint_Optimization___Problems___Still_life__as_a_Case_Study.pdf:\n",
      "Th us in the constrain t satisfaction comm unit y v ariable elimination is often disregarded In this pap er w e consider the c hallenging problem of nding stil llifes whic h are stable patterns of maxim um densit y in the game of life This academic problem has b een recen tly included in the CSPlib rep ository  and a dedicated w eb page  has b een set to main tain uptodate results In Bosc h None of them could solv e up to the n   problem within reasonable time Their b est results w ere obtained with a h ybrid approac h whic h com bines the t w o tec hniques and exploits the problem symmetries in order to reduce the searc h space With their algorithm they solv ed the n   case in ab out  da ys of cpu Smith 00 prop osed an in teresting alternativ e using pure constrain t programming tec hniques and solving the problem in its dual form In her w ork Smith could not impro v e the n   limit Although not explicitly men tioned these t w o w orks use algorithms with w orstcase time complexit solv e the problem with plain BE In Section  w e in tro duce the h ybrid algorithm with whic h w e obtained the results rep orted in Section In Section  w e discuss ho w the ideas explored in this article can b e extended to other domains Besides w e rep ort additional exp erimen tal results Finally  Section  giv es some\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for On_the_Practical_use_of_Variable_Elimination_in_Constraint_Optimization___Problems___Still_life__as_a_Case_Study.pdf:\n",
      "Th us in the constrain t satisfaction comm unit y v ariable elimination is often disregarded In this pap er w e consider the c hallenging problem of nding stil llifes whic h are stable patterns of maxim um densit y in the game of life This academic problem has b een recen tly included in the CSPlib rep ository  and a dedicated w eb page  has b een set to main tain uptodate results In Bosc h None of them could solv e up to the n   problem within reasonable time Their b est results w ere obtained with a h ybrid approac h whic h com bines the t w o tec hniques and exploits the problem symmetries in order to reduce the searc h space With their algorithm they solv ed the n   case in ab out  da ys of cpu Smith 00 prop osed an in teresting alternativ e using pure constrain t programming tec hniques and solving the problem in its dual form In her w ork Smith could not impro v e the n   limit Although not explicitly men tioned these t w o w orks use algorithms with w orstcase time complexit solv e the problem with plain BE In Section  w e in tro duce the h ybrid algorithm with whic h w e obtained the results rep orted in Section In Section  w e discuss ho w the ideas explored in this article can b e extended to other domains Besides w e rep ort additional exp erimen tal results Finally  Section  giv es some\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Introduction for On_the_Practical_use_of_Variable_Elimination_in_Constraint_Optimization___Problems___Still_life__as_a_Case_Study.pdf:\n",
      "Th us in the constrain t satisfaction comm unit y v ariable elimination is often disregarded In this pap er w e consider the c hallenging problem of nding stil llifes whic h are stable patterns of maxim um densit y in the game of life This academic problem has b een recen tly included in the CSPlib rep ository  and a dedicated w eb page  has b een set to main tain uptodate results In Bosc h None of them could solv e up to the n   problem within reasonable time Their b est results w ere obtained with a h ybrid approac h whic h com bines the t w o tec hniques and exploits the problem symmetries in order to reduce the searc h space With their algorithm they solv ed the n   case in ab out  da ys of cpu Smith 00 prop osed an in teresting alternativ e using pure constrain t programming tec hniques and solving the problem in its dual form In her w ork Smith could not impro v e the n   limit Although not explicitly men tioned these t w o w orks use algorithms with w orstcase time complexit solv e the problem with plain BE In Section  w e in tro duce the h ybrid algorithm with whic h w e obtained the results rep orted in Section In Section  w e discuss ho w the ideas explored in this article can b e extended to other domains Besides w e rep ort additional exp erimen tal results Finally  Section  giv es some conclusions and lines of future w ork  Preliminaries In this Section w e rst de ne the stilllife problem Next w e de ne the w eigh ted CSP framew ork and form ulate the stilllife as a w eigh ted CSP  Finally  w e review the main solving tec hniques for w eigh ted CSPS  Life and StillLife The ob jectiv e function is the sum of all functions in F  F  X f F f and the goal is to nd the instan tiation of v ariables that minimizes the ob jectiv e function Example  Consider a WCSP with four variables X  fx i g  i with domains\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Conclusion for On_the_Practical_use_of_Variable_Elimination_in_Constraint_Optimization___Problems___Still_life__as_a_Case_Study.pdf:\n",
      "In this pap er w e ha v e studied the applicabilit y of v ariable elimination to the problem of nding stil llifes Finding stilllifes is a c hallenging problem and dev eloping new solving tec hniques is an in teresting task p er se Besides w e ha v e de v elop ed an algorithm with whic h w e ha v e b een able to solv e up to the n  0 instance with whic h w e clearly impro v ed previous results The second con tribution of the pap er has a deep er insigh t Our algorithm uses recen t tec hniques based on v ariable elimination Since these tec hniques are little kno wn and rarely applied in the constrain ts comm unit y   On the pra ctical use of v ariable elimina tion Problem T B N C  T B D AC  T B D AC  H Y B T B F D AC  H Y B Sp ot 0 0   0 0 ssa   0  Figure  Exp erimen tal results in some W CSP instances with four di eren t algorithms Eac h column rep orts CPU time in seconds Sym b ol  indicates that a time limit of 00 seconds has b een reac hed the results presen ted in this pap er add new evidence of their p oten tial W e ha v e also sho wn that v ariable elimination can b e used b ey ond the academic stilllife problem b y pro viding exp erimen tal results in some unstructured realistic problems from di eren t domains Ac kno wledgmen ts The authors are grateful to Barbara Smith Neil Y ork eSmith and the anon ymous review ers for their useful commen ts at di eren t stages of the w ork rep orted in this article Marti Sanc hez kindly made the plots in Figure\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Enhanced Summary for On_the_Practical_use_of_Variable_Elimination_in_Constraint_Optimization___Problems___Still_life__as_a_Case_Study.pdf:\n",
      "Pap er w e consider the c hallenging problem of nding stil llifes whic h are stable patterns of maxim um densit y in the game of life. This academic problem has b een recen tly included in the CSPlib rep ository  and a dedicated w eb page  has been set to main tain uptodate results. None of them could solv e up to the n   problem within reasonable time. Their b est results w ere obtained with a h ybrid approac h whicH com bines the t w o tec hniques and exploits the problem symmetries in order to reduce the searc h space. The ideas explored in this article can b e extended to other domains. The authors are grateful to Neil Y orSmith and the anonous review  for their useful stages of the w ork stages at di eren t stages at the w rep orted stages of this article. The second con tribution of the pap er has a deep er insigh t. Our algorithm uses recen  techniques based on v ariable elimination since these tecHniques are little kno wn and rarely applied in the constrain ts comm unit y.\n",
      "--------------------------------------------------------------------------------\n",
      "[DEBUG] Comparison for On_the_Practical_use_of_Variable_Elimination_in_Constraint_Optimization___Problems___Still_life__as_a_Case_Study.pdf:\n",
      "Relevance Score: 58.28% - Common words: 88 / 151\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for Generalizing_Boolean_Satisfiability_III__Implementation.pdf:\n",
      "This is the third of three papers describing zap a satisﬁability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high performance solvers The fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the Boolean representation used our goal has been to deﬁne a representation in which this structure is apparent and can be exploited to improve computational performance The ﬁrst paper surveyed existing work that  exploited problem structure to improve the performance of satisﬁability engines and the second paper showed that this structure could be understood in terms of groups of permutations acting on individual clauses in any particular Boolean theory We conclude the series by discussing the techniques needed to implement our ideas and by reporting on their performance on a variety of problem instances 1 In this paper we discuss the implementation of a prover based on these ideas and describe its performance on pigeonhole parity and clique coloring problems These classes of problems are known to be exponentially diﬃcult for conventional Boolean satisﬁability engines and their formalization also highlights the groupbased nature of the reasoning involved From a technical point of view this is the most diﬃcult of the three zap papers we need to draw on the algorithms and theoretical constructions from zap2 and on results from com putational group theory  regarding their implementation Our overall plan for describing the implementation is as follows 1 Section 2 is a review of material from zap2 We begin in Section 21 by presenting both the Boolean satisﬁability algorithms that we hope to generalize and the basic algebraic ideas underlying zap Section 22 describes the grouptheoretic computations required by the zap implementation 2 Section 3 gives a brief  and necessarily incomplete  introduction to some of the ideas in computational group theory that we use 3 Sections 4 and 5 describe the implementations of the computations discussed in Sec tion 2 For each basic construction we describe the algorithm used and give an example of the computation in action If there is an existing implementation of some thing in the public domain system gap  we only provide a pointer to that implementation for concepts that we needed to implement from scratch additional detail is provided 4 Section 6 extends the basic algorithms of Section 5 to deal with unit propagation where we want to compute not a single unit clause instance but a list of all of the unit consequences of an augmented clause 5 Section 7 discusses the implementation of Zhang and Stickels  watched literal idea in our setting 6 Section 8 describes a technique that can be used to select among the possible resolvents of two augmented clauses This is functionality with no analog in a conventional prover where there is only a single ground reason for the truth or falsity of any given variable If the reasons are augmented clauses there may be a variety of ways in which ground instances of those clauses can be combined 7 After describing the algorithms we present experimental results regarding perfor mance in Sections 9 and 10 Section 9 reports on the performance of zaps individual algorithmic components while Section 10 contrasts zaps overall performance to that of its cnfbased predecessors1 Since our focus in this paper is on the algorithms 1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Abstract for Generalizing_Boolean_Satisfiability_III__Implementation.pdf:\n",
      "This is the third of three papers describing zap a satisﬁability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high performance solvers The fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the Boolean representation used our goal has been to deﬁne a representation in which this structure is apparent and can be exploited to improve computational performance The ﬁrst paper surveyed existing work that  exploited problem structure to improve the performance of satisﬁability engines and the second paper showed that this structure could be understood in terms of groups of permutations acting on individual clauses in any particular Boolean theory We conclude the series by discussing the techniques needed to implement our ideas and by reporting on their performance on a variety of problem instances 1 In this paper we discuss the implementation of a prover based on these ideas and describe its performance on pigeonhole parity and clique coloring problems These classes of problems are known to be exponentially diﬃcult for conventional Boolean satisﬁability engines and their formalization also highlights the groupbased nature of the reasoning involved From a technical point of view this is the most diﬃcult of the three zap papers we need to draw on the algorithms and theoretical constructions from zap2 and on results from com putational group theory  regarding their implementation Our overall plan for describing the implementation is as follows 1 Section 2 is a review of material from zap2 We begin in Section 21 by presenting both the Boolean satisﬁability algorithms that we hope to generalize and the basic algebraic ideas underlying zap Section 22 describes the grouptheoretic computations required by the zap implementation 2 Section 3 gives a brief  and necessarily incomplete  introduction to some of the ideas in computational group theory that we use 3 Sections 4 and 5 describe the implementations of the computations discussed in Sec tion 2 For each basic construction we describe the algorithm used and give an example of the computation in action If there is an existing implementation of some thing in the public domain system gap  we only provide a pointer to that implementation for concepts that we needed to implement from scratch additional detail is provided 4 Section 6 extends the basic algorithms of Section 5 to deal with unit propagation where we want to compute not a single unit clause instance but a list of all of the unit consequences of an augmented clause 5 Section 7 discusses the implementation of Zhang and Stickels  watched literal idea in our setting 6 Section 8 describes a technique that can be used to select among the possible resolvents of two augmented clauses This is functionality with no analog in a conventional prover where there is only a single ground reason for the truth or falsity of any given variable If the reasons are augmented clauses there may be a variety of ways in which ground instances of those clauses can be combined 7 After describing the algorithms we present experimental results regarding perfor mance in Sections 9 and 10 Section 9 reports on the performance of zaps individual algorithmic components while Section 10 contrasts zaps overall performance to that of its cnfbased predecessors1 Since our focus in this paper is on the algorithms 1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Introduction for Generalizing_Boolean_Satisfiability_III__Implementation.pdf:\n",
      "USA Abstract This is the third of three papers describing zap a satisﬁability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high performance solvers The fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the Boolean representation used our goal has been to deﬁne a representation in which this structure is apparent and can be exploited to improve computational performance The ﬁrst paper surveyed existing work that  exploited problem structure to improve the performance of satisﬁability engines and the second paper showed that this structure could be understood in terms of groups of permutations acting on individual clauses in any particular Boolean theory We conclude the series by discussing the techniques needed to implement our ideas and by reporting on their performance on a variety of problem instances 1 In this paper we discuss the implementation of a prover based on these ideas and describe its performance on pigeonhole parity and clique coloring problems These classes of problems are known to be exponentially diﬃcult for conventional Boolean satisﬁability engines and their formalization also highlights the groupbased nature of the reasoning involved From a technical point of view this is the most diﬃcult of the three zap papers we need to draw on the algorithms and theoretical constructions from zap2 and on results from com putational group theory  regarding their implementation Our overall plan for describing the implementation is as follows 1 Section 2 is a review of material from zap2 We begin in Section 21 by presenting both the Boolean satisﬁability algorithms that we hope to generalize and the basic algebraic ideas underlying zap Section 22 describes the grouptheoretic computations required by the zap implementation 2 Section 3 gives a brief  and necessarily incomplete  introduction to some of the ideas in computational group theory that we use 3 Sections 4 and 5 describe the implementations of the computations discussed in Sec tion 2 For each basic construction we describe the algorithm used and give an example of the computation in action If there is an existing implementation of some thing in the public domain system gap  we only provide a pointer to that implementation for concepts that we needed to implement from scratch additional detail is provided 4 Section 6 extends the basic algorithms of Section 5 to deal with unit propagation where we want to compute not a single unit clause instance but a list of all of the unit consequences of an augmented clause 5 Section 7 discusses the implementation of Zhang and Stickels  watched literal idea in our setting 6 Section 8 describes a technique that can be used to select among the possible resolvents of two augmented clauses This is functionality with no analog in a conventional prover where there is only a single ground reason for the truth or falsity of any given variable If the reasons are augmented clauses there may be a variety of ways in which ground instances of those clauses can be combined 7 After describing the algorithms we present experimental results regarding perfor mance in Sections 9 and 10 Section 9 reports on the performance of zaps individual algorithmic components while Section 10 contrasts zaps overall performance to that of its cnfbased predecessors1 Since our focus in this paper is on the algorithms 1 A description of zaps input language is contained in Appendix B 442 ZAP 3 Implementation needed by zap we report performance only for relatively theoretical examples that clearly involve groupbased reasoning Performance on a wider range of problem classes will be reported elsewhere 8 Concluding remarks appear in Section 11 Except for Section 3 proofs are generally deferred to Appendix A in the interests of main taining the continuity of our exposition Given the importance of computational group theory to the ideas that we will be presenting we strongly suggest that the reader work through the proofs in Section 3 of the paper This is a long and complex paper we make no apologies Zap is an attempt to synthesize two very diﬀerent ﬁelds each complex in its own right computational group theory and implementations of Boolean satisﬁability engines Computational group theory in addition to its inherent complexity is likely to be foreign to an AI audience Work on complete algorithms for Boolean satisﬁability has also become increasingly sophisticated over the past decade or so with the introduction of substantial and nonintuitive modiﬁcations to the original dpll algorithm such as relevancebounded learning  and watched literals  As we bring these two ﬁelds together we will see that a wide range of techniques from computational group theory is relevant to the problems of interest to us our goal is also not simply to translate dpll to the new setting but to show that all of the recent work on Boolean satisﬁability can be moved across In at least one case Lemma 526 we also need to extend existing computational group theory results\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Conclusion for Generalizing_Boolean_Satisfiability_III__Implementation.pdf:\n",
      "liquecoloring problems should not be thought of as unsatisﬁable instances of graphcoloring problems generally A particular instance of this problem class does not describe a speciﬁc graph that needs to be colored it says only that the graph contains an mclique and needs to be colored in m 1 colors An axiomatization of this problem is as follows We use eij to describe the graph cij to describe the coloring of the graph and qij to describe the embedding of the clique into the graph The graph has m nodes the clique is of size n  1 This is the version where there is a 3clique in a graph of size four and we are trying to use just two colors The ﬁrst group is the symmetry over colors alone the second that over the elements of the clique and the third the symmetry over nodes The axiomatization is identical to that presented earlier Note that although there is a common symmetry in this problem the axiomatization obscures that in some sense since we have only included the relevant symmetry or symmetries in any particular axiom Times to solution for zap and zChaff are shown in  As might be expected zChaff is scaling exponentially zap appears to be scaling as n85 In order to allow the data to be presented along a single axis these problem instances were selected so that the clique size was one smaller than the graph size  shows the number of nodes expanded by the two systems Once again the number of nodes expanded by zChaff is growing exponentially with problem size while the number expanded by zap is growing polynomially As with the pigeonhole problem we see that the short proofs whose existence is guaranteed by the theory can be found in practice Figures 14 and 15 display zaps performance on a somewhat wider range of problem instances where the clique and graph sizes are allowed to vary independently The number of nodes expanded was in general c  g2 13c g  14 2 where c is the size of the clique and g the size of the graph There were a handful of outliers most notably the c  11 g  13 instance which expanded a larger number of nodes The other exceptions all expanded fewer nodes With regard to total CPU time  the time appears to be scaling as 389 Once again c  11 g  13 is an outlier but polynomial performance is observed generally To the best of our knowledge zap is the ﬁrst system to exhibit polynomial performance on this problem class as we have remarked most other approaches have been proven to scale exponentially 104 Related Work Finally we compare our experimental results to those obtained using other systems that attempt to exploit problem structure to improve the performance of satisﬁability solvers This section provides a highlevel summary of experimental results for a number of these 505 Dixon Ginsberg Hofer Luks  Parkes 1 10 100 1000 10000 100000 1e06 1e07 4 6 8 10 12 14 16 18 nodes graph size  zap zchaff  Nodes expanded in the clique problems eﬀorts and compares these results with zap on the benchmark problems described in the previous sections Recall that our benchmark problems are all highly structured but each has a very dif ferent type of structure Theoretically these problems all allow polynomialtime solutions However neither of these solutions constitutes a practical general purpose solver We ran a number of solvers on the benchmark problems obtaining the following results pigeonhole Tseitin clique coloring zap P nlog n P zChaff E E E pbchaff P E E eqsatz E E E march eq\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[DEBUG] Enhanced Summary for Generalizing_Boolean_Satisfiability_III__Implementation.pdf:\n",
      "This is the third of three papers describing zap a satisﬁability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high performance solvers. The fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the Boolean representation used. We conclude the series by discussing the techniques needed to implement our ideas and by reporting on their performance on a variety of problem instances. This is a long and complex paper we make no apologies. Zap is an attempt to synthesize two very diﬀerent ﬁelds each complex in its own right. Computational group theory in addition to its inherent complexity is likely to be foreign to an AI audience. We see that a wide range of techniques from computational group theory is relevant to the problems of interest to us. It is also not simply to translate dpll to the setting of our setting but to show that all of the work on Boolean satisability can be moved across Lemma 526 at least in at least one case. We need to extend existing computational group theories to extend new new problems that need to be solved. The results of this paper do not describe a particular class of graphcoloring problems. A particular instance of this problem does not generally be specioloring.\n",
      "--------------------------------------------------------------------------------\n",
      "[DEBUG] Comparison for Generalizing_Boolean_Satisfiability_III__Implementation.pdf:\n",
      "Relevance Score: 31.47% - Common words: 79 / 251\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ]
  }
 ]
}
