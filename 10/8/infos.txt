Introduction: Submitted 1004 published 1205 Relational Dynamic Bayesian Networks Sumit Sanghai SANGHAICSWASHINGTONEDU Pedro Domingos PEDRODCSWASHINGTONEDU Daniel Weld WELDCSWASHINGTONEDU Department of Computer Science and Engineering University of Washington Abstract Stochastic processes that involve the creation of objects and relations over time are widespread but relatively poorly studied For example accurate fault diagnosis in factory assembly processes requires inferring the probabilities of erroneous assembly operations but doing this efﬁciently and accurately is difﬁcult Modeled as dynamic Bayesian networks these processes have discrete vari ables with very large domains and extremely high dimensionality In this paper we introduce relational dynamic Bayesian networks  which are an extension of dynamic Bayesian net works  to ﬁrstorder logic RDBNs are a generalization of dynamic probabilistic relational models  which we had proposed in our previous work to model dynamic uncertain do mains We ﬁrst extend the RaoBlackwellised particle ﬁltering described in our earlier work to RDBNs Experiments show these two methods greatly outperform standard particle ﬁltering on the task of assembly plan execution monitoring 1 Introduction Sequential phenomena abound in the world and uncertainty is a common feature of them Dynamic Bayesian networks  one of the most powerful representations available for such phenomena represent the state of the world as a set of variables and model the probabilistic dependencies of the variables within and between time steps  While a major advance over previous approaches DBNs are essentially propositional with no notion of objects or relations hence DBNs are unable to compactly represent many realworld domains For example manu facturing plants assemble complex artifacts eg cars computers aircraft from large numbers of component parts using multiple kinds of machines and operations Capturing such a domain in a DBN would require exhaustively representing all possible objects and relations among them which is impractical Formalisms that can represent objects and relations as opposed to just variables have a long history in AI Recently signiﬁcant progress has been made in combining them with a principled treatment of uncertainty In particular probabilistic relational models or PRMs  are an extension of Bayesian networks that allows reasoning with classes objects and relations Recently we proposed dynamic probabilistic relational models   which combine PRMs and DBNs to allow reasoning with classes objects and relations in a dynamic environment We also developed a relational Rao Blackwellized particle ﬁltering mechanism for state monitoring in DPRMs c2005 AI Access Foundation In this paper we introduce relational dynamic Bayesian networks  which extend DBNs to ﬁrstorder  domains RDBNs subsume DPRMs and have several advantages over them including greater simplicity and expressivity
Problem and Solution: No problem or solution section found.
Conclusion: Our algo rithms use the same amount of memory as PF but require additional time  to do smoothing Thus in our experiments the number of particles used by standard PF SPF and ASPF are 100000 50000 and 20000 respectively One can see that PF tends to diverge very quickly  while ASPF performs best and its approximation to the marginal distribution is close to the actual distribution Although the abstrac tion smoothing algorithm has low error we observe in the graph that the error increases with time We attribute this growth to the fact that the effective dimension of the assembly domain increases over time as new  relations are created making it increasingly difﬁcult to approx imate the distribution with a ﬁxed number of particles  shows the results of experiments for varying fault probabilities From the two ﬁgures we can conclude that the performance of the 784 RELATIONAL DYNAMIC BAYESIAN NETWORKS 0 005 01 015 02 025 03 035 04 0 2000 4000 6000 8000 10000 KL Divergence Time Step 2540 2060 PF1 SPF1 ASPF1 PF10 SPF10 ASPF10  ASPF predicts the marginal distributions most accurately for varying fault probabilities 1 10 and 1000 objects standard PF degrades with increasing fault probability and with the number of objects while ASPF remains almost unaffected Next we report experiments when Assumption A1 holds and compare ASPS with RaoBlackwe llized particle ﬁltering The experiment was performed on 1000 objects with fault probability pf  1   shows the mean KL divergence between the approximate marginal distri butions and the true ones We can see that the difference between the KL divergence of ASPS and the KL divergence of RBPF is very small and this difference remains almost constant over time We conclude that the approximations underlying abstraction smoothing are quite good  shows that the KL divergence for ASPF is greater than RBPF by at most 001 and is on average around 0005 indicating that our approximations are quite good We also compare RBPF and ASPF when the number of relationships per object is increased  plots the KL divergence at the last step when the two algorithms are run on 1000 objects and 1 pf and varying number of objects per slot One can see that the difference is always less than 03 and the curve is quite stable 74 Computing Joint Distributions Figures 16 and 17 show the KL divergence of the full joint distribution of the state  for PF PF with abstraction smoothing  and PF using relational kernel density estimation  on experiments done with varying number of objects and fault probabilities respectively One can see that the estimates for the joint proba bility have greater KL divergence  and RKDE gives the best results From the experiments we conclude that RKDE can estimate the joint distribution WELD 75 Experimental Conclusions The following are the conclusions that we can draw from the experiments  All of our algorithms  are much more accurate than standard PF for inference in RDBNs using similar computational resources  RBPF is the best method when Assumption A1 holds and scales up to a small maximum number of relations per object per predicate  PF with abstraction smoothing unlike RBPF is applicable in all scenarios to estimate the marginal distributions and is quite accurate  For estimating joint distributions relational kernel density estimation outperforms PF with abstraction smoothing 8 Related Work In recent years much research has focused on extending Bayesian networks to domains with rela tional structure Approaches include stochastic logic programs  probabilistic relational models Friedman et al 1999 However there has been very limited work on extending these to temporal domains Dynamic objectoriented Bayesian networks   combine DBNs with OOBNs a predecessor of PRMs Unfortunately no efﬁcient inference methods were proposed for DOOBNs and they have not been evaluated experimentally Glesner and Koller  proposed the idea of adding the power of ﬁrstorder logic to DBNs However they only give procedures for constructing ﬂexible DBNs out of ﬁrstorder knowledge bases and do not consider inference or learning procedures Like DOOBNs these were also not evaluated experimentally Relational Markov models Anderson et al 2002 and logical hidden Markov models   are an extension of HMMs to ﬁrstorder domains In our previous work we introduced dynamic probabilistic relational domains