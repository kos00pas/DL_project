Introduction: Abstract Most exact algorithms for general par tially observable Markov decision processes  use a form of dynamic program ming in which a piecewiselinear and con vex representation of one value function is transformed into another We examine vari ations of the incremental pruning method for solving this problem and compare them to earlier algorithms from theoretical and em pirical perspectives We find that incremen tal pruning is presently the most efficient ex act method for solving POMDPs 1 INTRODUCTION Partially observable Markov decision processes  model decision theoretic planning problems in which an agent must make a sequence of decisions to maximize its utility given uncertainty in the effects of its actions and its current state Cassandra Kael bling  Littman 1994 White 1991 At any moment in time the agent is in one of a finite set of possible states S and must choose one of a finite set of possible actions A After taking action a E A from state s E S the agent receives immediate reward ra E Ǣand the agents state becomes some states with the probabil ity given by the transition function PrsJsa E  The agent is not aware of its current state and in stead only knows its information state x which is a probability distribution over possible states x is the probability that the agent is in state s This results in a new information state x7 defined by where xas  Przjs a EsES Prsls ax z Solving a POMDP means finding a policy 1r that maps each information state into an action so that the expected sum of discounted rewards is maximized 0 D 1 D 1 is the discount rate which controls how much future rewards count compared to nearterm re wards There are many ways to approach this prob lem based on checking which information states can be reached  search ing for good controllers  and using dynamic programming  Most exact algorithms for general POMDPs use a form of dynamic programming in which a piecewise linear and convex representation of one value func tion is transformed into another This includes algo rithms that solve POMDPs via value iteration  policy iteration  accelerated value iteration  structured representations  and ap proximation  Because dynamic programming updates are critical to such a wide ar ray of POMDP algorithms identifying fast algorithms is crucial Several algorithms for dynamicprogramming updates have been proposed such as one pass  exhaustive  linear support  and witness  Cheng  gave experimental evidence that the linear support algorithm is more efficient than the onepass algorithm Littman Cassandra and Kael bling  compared the exhaustive algorithm the linear support algorithm and the witness algorithm and found that except for tiny problems with approx imately 2 observations or 2 states which all three al gorithms could solve quickly witness was the fastest and had a number of superior theoretical properties Recently Zhang and Liu  proposed a new method for dynamicprogramming updates in POMDPS called incremental pruning In this paper we analyze the basic algorithm and a novel variation and com pare them to the witness algorithm We find that the incrementalpruningbased algorithms allow us to solve problems that could not be solved within reason able time limits using the witness algorithm 2 DP UPDATES The fundamental idea of the dynamicprogramming  update is to define a new value function V in terms of a given value function V Value functions are mappings from information states to expected dis counted total reward In valueiteration algorithms V incorporates one additional step of reward compared to V and in infinitehorizon algorithms V represents an improved approximation that is closer to the opti mal value function The function V maps information states to values and is defined by V  E z rax  Y L PrVxƇ  sES zEZ
Problem and Solution: No problem or solution section found.
Conclusion: 