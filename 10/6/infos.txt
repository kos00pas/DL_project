Introduction: Abstract We present a novel approach to the automatic acquisition of taxonomies or concept hierarchies from a text corpus The approach is based on Formal Concept Analysis  a method mainly used for the analysis of data ie for investigating and processing explicitly given information We follow Harris distributional hypothesis and model the context of a certain term as a vector repre senting syntactic dependencies which are automatically acquired from the text corpus with a lin guistic parser On the basis of this context information FCA produces a lattice that we convert into a special kind of partial order constituting a concept hierarchy The approach is evaluated by com paring the resulting concept hierarchies with handcrafted taxonomies for two domains tourism and ﬁnance We also directly compare our approach with hierarchical agglomerative clustering as well as with BiSectionKMeans as an instance of a divisive clustering algorithm However it is also well known that any knowledgebased system suffers from the socalled knowledge acquisition bottleneck ie the difﬁculty to actually model the domain in question In c 2005 AI Access Foundation All rights reserved CIMIANO HOTHO  STAAB order to partially overcome this problem we present a novel approach to automatically learning a concept hierarchy from a text corpus Making the knowledge implicitly contained in texts explicit is a great challenge For example Brewster Ciravegna and Wilks  have argued that text writing and reading is in fact a process of background knowledge maintenance in the sense that basic domain knowledge is assumed and only the relevant part of knowledge which is the issue of the text or article is mentioned in a more or less explicit way Actually knowledge can be found in texts at different levels of explicitness depending on the sort of text considered Handbooks textbooks or dictionaries for example contain explicit knowledge in form of deﬁnitions such as a tiger is a mammal or mammals such as tigers lions or elephants In fact some researchers have exploited such regular patterns to discover taxonomic or partof relations in texts In fact different methods have been proposed in the literature to address the problem of semi automatically deriving a concept hierarchy from text based on the distributional hypothesis Basi cally these methods can be grouped into two classes the similaritybased methods on the one hand and the settheoretical on the other hand Both methods adopt a vectorspace model and represent a word or term as a vector containing features or attributes derived from a certain corpus There is certainly a great divergence in which attributes are used for this purpose but typically some sort of syntactic features are used such as conjunctions appositions  or verbargument dependencies Hindle 1990 Pereira Tishby  Lee 1993 Grefenstette 1994 Faure  Nedellec 1998 The ﬁrst type of methods is characterized by the use of a similarity or distance measure in order to compute the pairwise similarity or distance between vectors corresponding to two words or terms in order to decide if they can be clustered or not Some prominent examples for this type of method have been developed by Hindle  Pereira et al  Grefenstette  Faure and Nedellec  Caraballo  as well as Bisson Nedellec and Canamero  Set theoretical approaches partially order the objects according to the inclusion relations between their attribute sets  In this paper we present an approach based on Formal Concept Analysis a method based on order theory and mainly used for the analysis of data in particular for discovering inherent rela tionships between objects described through a set of attributes on the one hand and the attributes themselves on the other In order to derive attributes from a certain corpus we parse it and extract verbprepositional phrase complement verbobject and verbsubject dependencies For each noun appearing as head of these argument positions we then use the corre sponding verbs as attributes for building the formal context and then calculating the formal concept lattice on its basis Though different methods have been explored in the literature there is actually a lack of compar ative work concerning the task of automatically learning concept hierarchies with clustering tech niques However as argued by Cimiano Hotho and Staab  ontology engineers need guide lines about the effectiveness efﬁciency and tradeoffs of different methods in order to decide which techniques to apply in which settings Thus we present a comparison along these lines between our 306 LEARNING CONCEPT HIERARCHIES FROM TEXT CORPORA USING FORMAL CONCEPT ANALYSIS
Problem and Solution: the values of all the above measures are normalized into the interval  The third problem requires smoothing of input data In fact when working with text corpora data sparseness is always an issue  A typical method to overcome data sparseness is smoothing  which in essence consists in assigning nonzero probabil ities to unseen events For this purpose we apply the technique proposed by Cimiano Staab and Tane  in which mutually similar terms are clustered with the result that an occurrence of an attribute with the one term is also counted as an occurrence of that attribute with the other term As similarity measures we examine the Cosine Jaccard L1 norm JensenShannon divergence and Skew Divergence measures analyzed and described by Lee  os  P v ar g 2V P P  q In particular we implemented these measures using the variants relying only on the elements v ar g common to t 1 and t 2 as described by Lee  Strictly speaking the JensenShannon as well as the Skew divergences are dissimilarity functions as they measure the average information loss when using one distribution instead of the other In fact we transform them into similarity measures as k f where k is a constant and f the dissimilarity function in question We cluster all the terms which are mutually similar with regard to the similarity measure in question counting more attributeobject pairs than are actually found in the text and thus obtaining also nonzero frequencies for some attributeobject pairs that do not appear literally in the corpus The overall result is thus a smoothing of the relative frequency landscape by assigning some nonzero relative frequencies to combinations of verbs and objects which were actually not found in the corpus Here follows the formal deﬁnition of mutual similarity Deﬁnition 5  Two terms n 1 and n 2 are mutually similar iff n 2  ar g max n 0 sim and n 1  ar g max n 0 sim 313 CIMIANO HOTHO  STAAB  Examples of lattices automatically derived from tourismrelated texts without smoothing  and with smoothing  According to this deﬁnition two terms n 1 and n 2 are mutually similar if n 1 is the most similar term to n 2 with regard to the similarity measure in question and the other way round Actually the deﬁnition is equivalent to the reciprocal similarity of Hindle    shows an example of a lattice which was automatically derived from a set of texts acquired from  as well as  a web page contain ing information about the history accommodation facilities as well as activities of Mecklenburg Vorpommern a region in northeast Germany We only extracted verbobject pairs for the terms in  and used the conditional probability to weight the signiﬁcance of the pairs For excursion no dependencies were extracted and therefore it was not considered when computing the lattice The corpus size was about a million words and the threshold used was t  0005 Assuming that car and bike are mutually similar they would be clustered ie car would get the attribute startable and bike the attribute needable The result here is thus the lattice in   where car and bike are in the extension of one and the same concept 5 Evaluation In order to evaluate our approach we need to assess how good the automatically learned ontologies reﬂect a given domain One possibility would be to compute how many of the superconcept relations in the automatically learned ontology are correct
Conclusion: on of the clusters created in each step The authors present the results of their system in terms of cluster accuracy in dependency of percentage of the corpus used Caraballo  also uses clustering methods to derive an unla beled hierarchy of nouns by using data about conjunctions of nouns and appositions collected from the Wall Street Journal corpus Interestingly in a second step she also labels the abstract concepts of the hierarchy by considering the Hearst patterns  in which the children of the concept in question appear as hyponyms The most frequent hypernym is then chosen in order to label the concept At a further step she also compresses the produced ontological tree by eliminating internal nodes without a label The ﬁnal ontological tree is then evaluated by presenting a random choice of clusters and the corresponding hypernym to three human judges for validation Bisson et al  present an interesting framework and a corresponding workbench  MoK  allowing users to design conceptual clustering methods to assist them in an ontology building task In particular they use bottomup clustering and compare different similarity measures as well as different pruning parameters In earlier work we used collocation statistics to learn relations between terms using a modi ﬁcation of the association rules extraction algorithm However these relations were not inherently taxonomic such that the work described in this paper can not be di 330 LEARNING CONCEPT HIERARCHIES FROM TEXT CORPORA USING FORMAL CONCEPT ANALYSIS rectly compared to it Maedche Pekar and Staab  examined different supervised techniques based on collocations to ﬁnd the appropriate hypernym for an unknown term reaching an accuracy of around 15 using a combination of a tree ascending algorithm and kNearestNeighbors as well as the Skew Divergence as similarity measure These results are neither comparable to the task at hand Recently Reinberger and Spyns  have presented an application of clustering tech niques in the biomedical domain They evaluate their clusters by directly comparing to the UMLS thesaurus Their results are very low 317 precision depending on the corpus and clustering tech nique and comparable to the results we obtained when comparing our clusters directly with our gold standards and which are not reported in this paper though Furthermore there is quite a lot of work related to the use of linguistic patterns to discover certain ontological relations from text Hearsts seminal approach aimed at discovering taxonomic relations from electronic dictionaries  The precision of the isarelations learned is 61106 5755 when measured against WordNet as gold standard Hearsts idea has been reapplied by different researchers with either slight variations in the patterns used Iwanska et al 2000 in very speciﬁc domains Ahmad et al 2003 to acquire knowledge for anaphora resolution  or to discover other kinds of semantic relations such as partof relations  or causation relations  The approaches of Hearst and others are characterized by a  high precision in the sense that the quality of the learned relations is very high However these approaches suffer from a very low recall which is due to the fact that the patterns are very rare As a possible solution to this problem in the approach of Cimiano Pivk SchmidtThieme and Staab  Hearst patterns matched in a corpus and on the Web as well as explicit information derived from other resources and heuristics are combined yielding better results compared to considering only one source of evidence on the task of learning superconcept relations In general to overcome such data sparseness problems researchers are more and more resorting to the WWW as for example Markert Modjeska and Nissim In their approach Hearst patterns are searched for on the WWW by using the Google API in order to acquire background knowledge for anaphora resolution Agirre Ansa Hovy and Martinez  download related texts from the Web to enrich a given ontology Cimiano Handschuh and Staab  as well as Cimiano Ladwig and Staab  have used the Google API to match Hearstlike patterns on the Web in order to  ﬁnd the best concept for an unknown instance as well as  the appropriate superconcept for a certain concept in a given ontology  Velardi Fabriani and Missikoff  present the OntoLearn system which discovers i the domain concepts relevant for a certain domain ie the relevant terminology ii named entities iii vertical isa or taxonomic relations as well as iv certain relations between concepts based on speciﬁc syntactic relations In their approach a vertical relation is established between a term t 1 and a term t 2 ie isa if t 2 can be gained out of t 1 by stripping of the latters prenominal modiﬁers such as adjectives or modifying nouns Thus a vertical relation is for example estab lished between the term international credit card and the term credit card ie isa In a further paper  the main focus is on the task of word sense disambiguation ie of ﬁnding the correct sense of a word with respect to a general ontology or lexical database In particular they present a novel algorithm called SSI relying on the structure of the general ontology for this purpose Sanderson and Croft  describe an interesting approach to automatically derive a hierarchy by considering the document a certain term appears in as context In particular they present a documentbased deﬁnition of subsumption according to which a certain term t 1 is more special than a term t 2 if t 2 also appears in all the documents in which t 1 appears Formal Concept Analysis can be applied for many tasks within Natural Language Processing Priss  for example mentions several possible applications of FCA in analyzing linguistic structures lexical semantics and lexical tuning Sporleder  and Petersen  apply FCA to yield more concise lexical inheritance hierarchies with regard to morphological features such as numerus gender etc Basili Pazienza and Vindigni  apply FCA to the task of learning subcategorization frames from corpora